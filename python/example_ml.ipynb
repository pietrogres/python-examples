{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182046de",
   "metadata": {},
   "source": [
    "# Machine Learning in Python manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee1477",
   "metadata": {},
   "source": [
    "<a name=\"contents\"></a>\n",
    "# Contents\n",
    "\n",
    "- [Configuration](#conf)\n",
    "- [Functions](#fnc)\n",
    "- [Data Import](#imp)\n",
    "- [Data Exploration](#expl)\n",
    "- [Feature Engineering](#eng)\n",
    "    - [Missing Values](#miss)\n",
    "    - [Outliers](#out)\n",
    "    - [Feature Encoding](#enc)\n",
    "    - [Feature Scaling](#scal)\n",
    "    - [Feature Selection](#sel)\n",
    "    - [Dimensionality Reduction](#red)\n",
    "    - [Preprocessing Pipelines](#pipe)\n",
    "- [Machine Learning models](#ml)\n",
    "    - [Models](#mlmodels)\n",
    "    - [Prediction intervals](#mlint)\n",
    "- [Deep Learning models](#dl)\n",
    "- [Hyperparameters Tuning](#hyper)\n",
    "    - [Cross Validation](#cv)\n",
    "    - [Optuna](#optuna)\n",
    "- [Performances](#perf)\n",
    "    - [SHAP](#shap)\n",
    "    - [Lift curve](#lift)\n",
    "- [TensorFlow](#tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3c6e7",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"conf\"></a>\n",
    "# Configuration\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db275d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:52:52.011571Z",
     "start_time": "2022-09-28T15:52:51.992364Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import random as rnd\n",
    "from typing import Dict, List\n",
    "from datetime import timedelta\n",
    "from operator import attrgetter\n",
    "\n",
    "import joblib\n",
    "import unidecode\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import haversine as hs\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.datasets import load_iris, load_diabetes, load_digits, load_breast_cancer, fetch_california_housing, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, IsolationForest, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, precision_recall_curve, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV  # , HalvingGridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from xgboost import XGBClassifier, XGBRegressor, plot_importance\n",
    "# from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# import tensorflow as tfb\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from functions import plot_histrogram, plot_box, plot_violin, plot_distribution, check_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8927c3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a04642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:20:08.665991Z",
     "start_time": "2022-09-28T15:20:06.021912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f42d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:20:08.692445Z",
     "start_time": "2022-09-28T15:20:08.665991Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38804bcd",
   "metadata": {},
   "source": [
    "### Logging & alerts configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe9470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:20:08.714292Z",
     "start_time": "2022-09-28T15:20:08.692445Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    # filename=logging_file,\n",
    "    level=logging.DEBUG,  # logging.INFO\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    # filemode='w'\n",
    ")\n",
    "\n",
    "logging.info('Starting notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logging for matplotlib font\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d613615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:20:08.748600Z",
     "start_time": "2022-09-28T15:20:08.715297Z"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "alert('Configuration Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37548af0",
   "metadata": {},
   "source": [
    "Data generation and import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89948fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:20:16.398788Z",
     "start_time": "2022-09-28T15:20:16.367343Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"\n",
    "    Generates data sample as seen in \"Prediction Intervals for Gradient Boosting Regression\"\n",
    "    (https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    f = lambda u: u * np.sin(u)\n",
    "\n",
    "    #  first the noiseless case\n",
    "    X_train = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T\n",
    "    X_train = X_train.astype(np.float32)\n",
    "\n",
    "    # observations\n",
    "    y_train = f(X_train).ravel()\n",
    "    dy = 1.5 + 1.0 * np.random.random(y_train.shape)\n",
    "    noise = np.random.normal(0, dy)\n",
    "    y_train += noise\n",
    "    y_train = y_train.astype(np.float32)\n",
    "\n",
    "    # mesh the input space for evaluations of the real function, the prediction and\n",
    "    # its MSE\n",
    "    X_test = np.atleast_2d(np.linspace(0, 10.0, 1000)).T\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = f(X_test).ravel()\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88f30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:53:40.772666Z",
     "start_time": "2022-09-28T15:53:40.533748Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# # SINGLE PLOT\n",
    "# # fpr, tpr, auc_thresholds = roc_curve(y, y_proba)\n",
    "# # plot_roc_curve(fpr, tpr)\n",
    "# plot_roc_curve(y, y_proba)\n",
    "\n",
    "# # p, r, thresholds = precision_recall_curve(y, y_proba)\n",
    "# # plot_precision_recall_vs_threshold(p, r, thresholds)\n",
    "# plot_precision_recall_vs_threshold(y, y_proba)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# # MULTIPLE PLOTS FOR THE SAME MODEL\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "\n",
    "# # fpr, tpr, auc_thresholds = roc_curve(y, y_proba)\n",
    "# # plot_roc_curve(fpr, tpr, ax=axs[0])\n",
    "# plot_roc_curve(y, y_proba, ax=axs[0])\n",
    "\n",
    "# # p, r, thresholds = precision_recall_curve(y, y_proba)\n",
    "# # plot_precision_recall_vs_threshold(p, r, thresholds, ax=axs[1])\n",
    "# plot_precision_recall_vs_threshold(y, y_proba, ax=axs[1])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MULTIPLE PLOTS FOR DIFFERENT MODELS\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "\n",
    "# # first model\n",
    "# fpr, tpr, auc_thresholds = roc_curve(y, y_proba)\n",
    "# plot_roc_curve(fpr, tpr, label='First model', ax=axs[0])\n",
    "# # second model\n",
    "# fpr, tpr, auc_thresholds = roc_curve(y, y_proba_bis)\n",
    "# plot_roc_curve(fpr, tpr, label='Second model', ax=axs[0])\n",
    "\n",
    "# p, r, thresholds = precision_recall_curve(y, y_proba)\n",
    "# plot_precision_recall_vs_threshold(p, r, thresholds, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cb647",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"imp\"></a>\n",
    "# Data Import\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2700c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T09:31:58.923166Z",
     "start_time": "2022-09-23T09:31:58.905027Z"
    }
   },
   "outputs": [],
   "source": [
    "root_path = re.sub('(?<=python_examples)(.+)', '', os.getcwd())\n",
    "python_path = os.path.join(root_path, 'python')\n",
    "data_path = os.path.join(python_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d0d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T09:32:14.023641Z",
     "start_time": "2022-09-23T09:32:13.755138Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, 'housing.csv'))\n",
    "# you can specify separator, columns types, null values\n",
    "df = pd.read_csv(os.path.join(data_path, 'housing.csv'), sep=',', dtype={'ocean_proximity':str}, na_values=['null','Null','NULL','nan','NaN','NAN'], keep_default_na=False)\n",
    "\n",
    "print(f'{len(df)} rows, {len(df.columns)} features')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9f7e5",
   "metadata": {},
   "source": [
    "#### pubicly available datasets\n",
    "\n",
    "Scikit learn provides a number of publicly available datasets which can be imported using sklearn methods\n",
    "\n",
    "**References**\n",
    "- https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "- https://inria.github.io/scikit-learn-mooc/python_scripts/trees_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris\n",
    "dataset = load_iris()\n",
    "print(f'{dataset.data.shape[0]} rows, {dataset.data.shape[1]} features')\n",
    "display(pd.DataFrame(dataset.data, columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']).head(1))\n",
    "\n",
    "# olivetti faces\n",
    "dataset = fetch_olivetti_faces()\n",
    "print(f'{dataset.images.shape[0]} images of shape {dataset.images.shape[1]}x{dataset.images.shape[2]} belonging to {len(np.unique(dataset.target))} people')\n",
    "\n",
    "# california housing\n",
    "dataset = fetch_california_housing(as_frame=True)\n",
    "print(f'{len(dataset.frame)} rows, {len(dataset.frame.columns)} features')\n",
    "display(dataset.frame.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fd530",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"expl\"></a>\n",
    "# Data Exploration\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d118c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T09:43:04.932160Z",
     "start_time": "2022-09-23T09:43:04.785309Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df.describe(include=['object'])  # ['int32','flat64','object','category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08c2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T09:43:10.777523Z",
     "start_time": "2022-09-23T09:43:10.755976Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f158a0",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a2177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T14:06:04.535234Z",
     "start_time": "2022-09-23T14:05:59.965451Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot_histrogram(df)\n",
    "# plot_box(df)\n",
    "# plot_violin(df)\n",
    "plot_distribution(df, cols=['longitude','total_rooms'], plots=['hist','box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575e103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T14:04:37.936634Z",
     "start_time": "2022-09-23T14:04:37.893976Z"
    }
   },
   "outputs": [],
   "source": [
    "# outliers\n",
    "check_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e695b",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22392da",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'median_house_value'\n",
    "\n",
    "print(f'{feature} - skewness: {round(df[feature].skew(), 4)}')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.histplot(df[feature], kde=True, bins=100)\n",
    "plt.title(f'{feature} - skewness: {round(df[feature].skew(), 4)}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e5645",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Correlation can be measured with correlation or or heatmap matrixes.  \n",
    "Correlation can be computed directly in pandas with the `corr()` method and then can be easily inspected thanks to searborn `heatmap()` function.\n",
    "\n",
    "**Note** that correlation only measures linear correlation hence it may completely miss out on nonlinear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46abfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).corr()\n",
    "display(corr_matrix)\n",
    "\n",
    "# to extract correlation with target feature\n",
    "corr_matrix['median_house_value'].reset_index().sort_values(by='median_house_value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.heatmap(corr_matrix.abs(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789f101",
   "metadata": {},
   "source": [
    "### Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfce886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:46:57.440689Z",
     "start_time": "2022-06-14T14:46:54.308818Z"
    }
   },
   "outputs": [],
   "source": [
    "attributes = ['median_house_value','median_income','total_rooms','housing_median_age']\n",
    "pd.plotting.scatter_matrix(df[attributes], figsize=(15, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d0418",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"eng\"></a>\n",
    "# Feature Engineering\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map feature values using dictionary\n",
    "df['total_bedrooms'] = df['total_bedrooms'].replace({'':np.nan})\n",
    "\n",
    "# modify values based on rule\n",
    "df.loc[df.total_bedrooms == '', 'total_bedrooms'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change feature type\n",
    "df['total_bedrooms'] = df['total_bedrooms'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ce983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and target features\n",
    "target_col = 'median_house_value'\n",
    "X = df.drop(target_col, axis=1)\n",
    "y = df[target_col]\n",
    "\n",
    "print(X.shape)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a823ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify numeric and categorical features\n",
    "num_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_cols = X.select_dtypes(include=num_types).columns\n",
    "cat_cols = X.select_dtypes(['object']).columns.tolist()\n",
    "print(f'Found {len(num_cols)} numeric columns and {len(cat_cols)} categorical columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ae7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8f45d",
   "metadata": {},
   "source": [
    "<a name=\"miss\"></a>\n",
    "## Missing Values\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "**Tips**:\n",
    "- use kNN to impute missing values in numerical features (always, it is better than nn for continuous features)\n",
    "- use deep learning imputation techniques when dealing with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d10c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T09:51:49.327667Z",
     "start_time": "2022-06-15T09:51:49.314488Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info(f'{len([k for k,v in dict(X.isna().sum()).items() if v > 0])} features containing missing values')\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d80413",
   "metadata": {},
   "source": [
    "You should never consider dropping missing values as part of your data preparation step.  \n",
    "Even knowing that some kind of information is missing for our data is a piece of information and you can use it to gain valuable insight on data\n",
    "\n",
    "Anyway you can consider the following methods to drop missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop rows with missing values\n",
    "X.dropna(subset=['total_bedrooms'])\n",
    "# to drop the entire column\n",
    "X.drop('total_bedrooms', axis=1).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c04072",
   "metadata": {},
   "source": [
    "Standard missing values imputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrooms_median = X['total_bedrooms'].median()\n",
    "X['total_bedrooms'] = X['total_bedrooms'].fillna(bedrooms_median)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8438fa",
   "metadata": {},
   "source": [
    "Imputing missing valus with kNN can be done with the `KNNImputer`\n",
    "\n",
    "**Note** that to train a `KNNImputer` train data must be numeric hence all categorical features have to be removed or encoded in order to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290a143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T15:15:43.570000Z",
     "start_time": "2022-06-14T15:15:43.038494Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "imputed_vals = knn_imputer.fit_transform(X[num_cols])\n",
    "X = pd.concat([pd.DataFrame(imputed_vals, columns=num_cols), X[cat_cols]], axis=1)\n",
    "assert X.isna().sum().max() == 0, 'ERROR! Found columns with missing values'\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2df0f1",
   "metadata": {},
   "source": [
    "<a name=\"out\"></a>\n",
    "# Outliers\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39890801",
   "metadata": {},
   "source": [
    "### z-Score\n",
    "\n",
    "In statistics, the z-score it's a measure of how many standard deviations a raw score (meaning a data point, a sample) is above or below the population mean.  \n",
    "It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation, hence, given a data point $x$ drawn from a population $N(μ,σ^{2})$ then the z-score for $x$ is computed as:\n",
    "$$z = \\frac{(x – μ)}{σ}$$\n",
    "\n",
    "For example, a z-score of 2.5 indicates that the $x$ lies $2.5σ$ away from $μ$.\n",
    "\n",
    "Remember that given a Gaussian distribution $N(μ,σ^{2})$:\n",
    "- 68% of the data points lie between $\\pmσ$ from $μ$\n",
    "- 95% of the data points lie between $\\pm2σ$ from $μ$\n",
    "- 99.7% of the data points lie between $\\pm3σ$ from $μ$\n",
    "\n",
    "\n",
    "Usually $z=3$ is considered as a cut-off value to set a distinction between normal and anomalous data points.  \n",
    "Therefore, any z-score greater than $+3$ or less than $-3$ is considered as outlier which is pretty much similar to standard deviation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d05aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_cols = X.select_dtypes(include=num_types).columns\n",
    "cat_cols = X.select_dtypes(['object']).columns.tolist()\n",
    "print(f'Found {len(num_cols)} numeric columns and {len(cat_cols)} categorical columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the threshold for the z-score method\n",
    "z_score_th = 3\n",
    "\n",
    "# compute z-scores for all data points\n",
    "z_score = np.abs(stats.zscore(X[num_cols]))\n",
    "\n",
    "# filter data points based on their z-scores\n",
    "X_outliers = X[(z_score[num_cols] >= z_score_th).any(axis=1)]\n",
    "X_non_outliers = X[(z_score[num_cols] < z_score_th).all(axis=1)]\n",
    "\n",
    "# remove target values corresponding to outliers\n",
    "y_valid = y.loc[X_non_outliers.index]\n",
    "\n",
    "assert all(X_non_outliers.index == y_valid.index), 'ERROR! Mismatching features and target'\n",
    "print(f'Removed {len(X_outliers)} outlier observations, considering {len(X_non_outliers)} valid observations')\n",
    "X_non_outliers.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67287105",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "\n",
    "Isolation Forest is an unsupervised technique for identifying outliers using binary trees to detect anomalies, resulting in a linear time complexity and low memory usage that is well-suited for processing large datasets.  \n",
    "The idea behind isolation forest is to identify as outliers those data points that are easily separable from the remaining data by linear splits, careless of whether those points are contained in data distribution or are outside.\n",
    "\n",
    "It works by training binary trees on random samples drawn from the population. At each step, a random feature and a random threshold are selected to split data and these steps are repeated till each data point is completely isolated or until the max depth (if defined) is reached.  \n",
    "Then, for each data point an anomaly score is computed based on the depth of the tree required to arrive at that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49668de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define isolation forest model\n",
    "if_model = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.001), random_state=123)\n",
    "if_model.fit(X[num_cols])\n",
    "\n",
    "# compute anomaly scores\n",
    "X['anomaly_score'] = if_model.predict(X[num_cols])\n",
    "\n",
    "# filter data points based on their anomaly scores\n",
    "X_outliers = X[X['anomaly_score'] == -1]\n",
    "X_non_outliers = X[X['anomaly_score'] == 1]\n",
    "\n",
    "# remove target values corresponding to outliers\n",
    "y_valid = y.loc[X_non_outliers.index]\n",
    "\n",
    "assert all(X_non_outliers.index == y_valid.index), 'ERROR! Mismatching features and target'\n",
    "print(f'Removed {len(X_outliers)} outlier observations, considering {len(X_non_outliers)} valid observations')\n",
    "X_non_outliers.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80eb45",
   "metadata": {},
   "source": [
    "<a name=\"enc\"></a>\n",
    "## Feature Encoding\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Although some machine learning models are able to deal with categorical values, most of the machine learning models can only work with numerical values.  \n",
    "Feature encoding refers to the process of transforming categorical feature values into number that are more easily processed by a machine learning algorithm.\n",
    "\n",
    "Note that there are 2 different types of categorical data which should be treated differently:\n",
    "- ordinal data: data that comprises a finite set of discrete values with an order (es low, medium, high)\n",
    "- nominal data: data that comprises a finite set of discrete values with no relationship between them (es india, congo, china)\n",
    "\n",
    "**NOTE** apart from ordinal and nominal data there is actually also **cyclic** categorical data. Cyclic data refers to data that have a cyclic nature such as days of the week, months names...  \n",
    "A common method for encoding cyclical data is to transform it into 2 dimensions using a sine and cosine transformation.\n",
    "\n",
    "Here is a list of techiques that can be used for feature encoding:\n",
    "- **Label Encoding**: it consists of substituting each group with a corresponding number and keeping such numbering consistent throughout the feature.  \n",
    "This technique should be used when the categorical feature is ordinal since converting data to numbers creates relationship between them that do not exist in nominal category data.\n",
    "- **Target Encoding**: it consists of substituting each group in a categorical feature with a numeric value based on the relationship between the category and the target variable (for example the mean).  \n",
    "Target Encoding is a powerful solution also because it avoids generating a high number of features, as is the case for One-Hot Encoding, keeping the dimensionality of the dataset as the original one, making it suitable for models like decision trees and gradient boosting.\n",
    "- **Leave-One-Out Encoding**: it encodes each category in a categorical variable with a numeric value based on the relationship between the category and the target variable excluding the current data point, which belongs to that category.  \n",
    "This approach is similar to Target Encoding and it addresses the fact that Target Encoding can lead to target leakage or overfitting.\n",
    "- **One-Hot Encoding**: it's consists of creating an additional feature for each group of the categorical feature and mark each observation belonging (value=1) or not (value=0) to that group.  \n",
    "This approach is able to encode categorical features properly, despite some minor drawbacks. Specifically, the presence of a high number of binary values is not ideal for distance-based algorithms, such as Clustering models.  \n",
    "In addition, the high number of additionally generated features can lead to the matrix being highly sparse (curse of dimensionality).\n",
    "- **Hash Encoding**: it is a technique to represent categories similarly to One-Hot Encoding as a sparse matrix but with a much lower dimensionality.  \n",
    "In feature hashing, a hashing function is applied to the category and then categories are represented using its indices\n",
    "- **Binary Encoding**: it is a combination of Hash and One-Hot Encoding.  \n",
    "It works really well where there is a high number of categories.\n",
    "\n",
    "\n",
    "**Libraries**\n",
    "- https://contrib.scikit-learn.org/category_encoders/index.html\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/handling-categorical-data-the-right-way-9d1279956fc6\n",
    "- https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\n",
    "- https://towardsdatascience.com/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de44ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_cols = X.select_dtypes(include=num_types).columns\n",
    "cat_cols = X.select_dtypes(['object']).columns.tolist()\n",
    "print(f'Found {len(num_cols)} numeric columns and {len(cat_cols)} categorical columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d368219",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "In scikit-learn label encoding of feature variables can be done using the OrdinalEncoder class. scikit-learn also offers the LabelEncoder class that is specifically build to take care of the encoding of target variable and should not be used to encoder feature variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eec895",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "encoded_vals = ordinal_encoder.fit_transform(X[cat_cols])\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), pd.DataFrame(encoded_vals, columns=cat_cols)], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42362ee",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70492465",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = ce.TargetEncoder()\n",
    "encoded_vals = target_encoder.fit_transform(X[cat_cols], y)\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), pd.DataFrame(encoded_vals, columns=cat_cols)], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7d3b4",
   "metadata": {},
   "source": [
    "### Leave-One-Out Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ee9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T16:16:08.657388Z",
     "start_time": "2022-08-29T16:16:08.640460Z"
    }
   },
   "outputs": [],
   "source": [
    "loo_encoder = ce.leave_one_out.LeaveOneOutEncoder(cols=cat_cols)\n",
    "encoded_vals = loo_encoder.fit_transform(X, y)\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), pd.DataFrame(encoded_vals, columns=cat_cols)], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e41142",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "In digital circuits and machine learning, a one-hot is a group of bits among which the legal combinations of values are only those with a single high bit (a 1) and all the others are low (0).\n",
    "\n",
    "One-Hot Encoding therefore refers to the encoding method by which a new feature is created for every distinct value in the categorical feature. It can be implemented in pandas with the `get_dummies()` or the `OneHotEncoder()` functions. The difference between the 2 is that OneHotEncoder saves the exploded categories into it’s object. This is extremely important in the case that the unique values for the category feature are different between train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding with pandas get_dummies\n",
    "encoded_vals = pd.get_dummies(X[cat_cols], drop_first=True)\n",
    "\n",
    "# encoded_df = df.drop(cat_cols, axis=1)\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), encoded_vals], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c28e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding with sklearn OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_vals = one_hot_encoder.fit_transform(X[cat_cols])\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), pd.DataFrame(encoded_vals)], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166e8d3",
   "metadata": {},
   "source": [
    "### Hash Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceac8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scikit-learn\n",
    "n_features = 5\n",
    "hash_encoder = FeatureHasher(n_features=n_features, input_type='string')\n",
    "encoded_vals = hash_encoder.transform(X[cat_cols].values)\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), pd.DataFrame(encoded_vals.toarray())], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcacc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using category_encoders library\n",
    "n_components = 5\n",
    "hash_encoder = ce.hashing.HashingEncoder(n_components=n_components)\n",
    "encoded_vals = hash_encoder.fit_transform(X[cat_cols])\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), encoded_vals], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219eee6",
   "metadata": {},
   "source": [
    "### Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_encoder = ce.BinaryEncoder(cols=cat_cols)\n",
    "encoded_vals = binary_encoder.fit_transform(X[cat_cols])\n",
    "\n",
    "X_encoded = pd.concat([X.drop(cat_cols, axis=1), encoded_vals], axis=1)\n",
    "print(f'Encoded dataframe results in {len(X_encoded)} rows and {len(X_encoded.columns)} columns')\n",
    "X_encoded.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894668eb",
   "metadata": {},
   "source": [
    "<a name=\"scal\"></a>\n",
    "## Feature Scaling\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Feature scaling refers to a family of techniques used in machine learning and data preprocessing that aim to bring different features of a dataset onto a similar scale.  \n",
    "The goal is to ensure that all features contribute equally to the learning process and prevent any particular feature from dominating or biasing the results.\n",
    "\n",
    "Among feature scaling techniques there are:\n",
    "- **Normalization**: it maps input data to the interval $[0,1]$ (or $[-1,1]$ if negative values are present) while keeping the original data distribution. This method is highly impacted by outliers.\n",
    "- **Standardization**: this technique assumes data is normally distributed and scales it so that the mean is 0 and the variance is 1. It is also referred to as **z-score** method. Note that this method is sensitive to outliers and to data skewness.\n",
    "- **Robust standardization**: similar to standardization technique but uses the mediand and inter quantile range (IQR) instead of mean and variance being therefore robust against outliers.\n",
    "- **Box-Cox transformation**: it belongs to a family of transformations called **power tranformations** and it aims to create a monotonic transformation of data using power functions to stabilize variance and make data more normally distributed.  \n",
    "- **Skewness transformations**: they refer to a wide range of functions and methods used to transform data in order to remove skewness. You should select the best method depending on the particular data distribution and skeweness type. \n",
    "\n",
    "These techniques help in several ways:\n",
    "- improved convergence: scaled data can help algorithms converge faster during training by preventing certain features from dominating the learning process due to their larger scales\n",
    "- equalized impact: they ensure that all features contribute equally to the learning process, regardless of their original scales\n",
    "- avoidance of numerical instability: they prevent numerical issues that can occur when features have extremely large or small values\n",
    "- interpret ability: they allow for easier interpretation and comparison of feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d89d7",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is a rescaling technique of data that maps original values to the range $[0, 1]$.  \n",
    "This transformation is simply obtained using sample min and max values as follows\n",
    "\n",
    "$$x_{scal}=\\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Normalization works well when standard deviation is small but it is highly impacted by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performing normalization of {len(num_cols)} numerical features...')\n",
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_feat = minmax_scaler.fit_transform(X[num_cols])\n",
    "X_scaled = pd.DataFrame(scaled_feat, columns=num_cols, index=X.index)\n",
    "# X[num_cols] = scaled_feat\n",
    "\n",
    "# to revert scaling\n",
    "minmax_scaler.inverse_transform(scaled_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626774e",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization, also known as Z-score normalization or Z-score scaling, is a technique that involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.  \n",
    "This is obtained with the following transformation\n",
    "\n",
    "$$x_{scal}=\\frac{x-\\bar{x}}{s}$$\n",
    "\n",
    "where $\\bar{x}$ is the sample mean and $s$ the sample standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performing standard scaling of {len(num_cols)} numerical features...')\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_feat = standard_scaler.fit_transform(X[num_cols])\n",
    "X_scaled = pd.DataFrame(scaled_feat, columns=num_cols, index=X.index)\n",
    "# X[num_cols] = scaled_feat\n",
    "\n",
    "# to revert scaling\n",
    "standard_scaler.inverse_transform(scaled_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032c242",
   "metadata": {},
   "source": [
    "### Robust standardization\n",
    "\n",
    "Standardization is a popular scaling technique that transforms the probability distribution for an input variable to a standard Gaussian (zero mean and unit variance). However it's important to note that standardization can produce skewed or biased data if the input variable contains outlier values.  \n",
    "To overcome this, the median and interquartile range (IQR) can be used when standardizing numerical input variables. This method is generally referred to as robust standardization or robust scaling. In formula\n",
    "$$x_{scal}=\\frac{x-x_{median}}{IQR}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performing robust scaling of {len(num_cols)} numerical features...')\n",
    "robust_scaler = RobustScaler()\n",
    "scaled_feat = robust_scaler.fit_transform(X[num_cols])\n",
    "X_scaled = pd.DataFrame(scaled_feat, columns=num_cols, index=X.index)\n",
    "# X[num_cols] = scaled_feat\n",
    "\n",
    "# to revert scaling\n",
    "robust_scaler.inverse_transform(scaled_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fccbd",
   "metadata": {},
   "source": [
    "### Box-Cox transformation\n",
    "\n",
    "The original Box-Cox tranformation is a one-dimensional transformation defined as\n",
    "\n",
    "$$y_{i}^{\\lambda}=\\begin{cases}\\frac{y_{i}^{\\lambda} - 1}{\\lambda} & \\lambda != 0, \\\\ \\log(y_{i}) & \\lambda = 0 \\end{cases}$$\n",
    "\n",
    "where the optimal $\\lambda$ is estimated via Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "The Box-Cox transformation is widely used for transforming features in order to make them more normally distributed. Note that when the original data distribution is far from being normal then this method might not work as you wish to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy stats implementation of boxcox method estimates the optimal lambda but does not work with negative x\n",
    "scaled_feat, lmbda = sp.stats.boxcox(X[num_cols])\n",
    "\n",
    "# if you have data between -1 and 0 then you have to use scipy special implementation which however does not provide a method to estimate lambda\n",
    "lam = .15\n",
    "scaled_feat = sp.special.boxcox1p(X[num_cols], lam)\n",
    "\n",
    "# to revert scaling\n",
    "sp.special.inv_boxcox1p(scaled_feat, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000aa31",
   "metadata": {},
   "source": [
    "### Skeweness transformations\n",
    "\n",
    "Skewness is a measure of asymmetry in a distribution of data.\n",
    "Skewed data can be of 2 types\n",
    "- right-skewed: also referred to as positively-skewed data, it's when data is clustered at low values\n",
    "- left-skewed: also referred to as negatively-skewed data, it's when data is clustered at high values\n",
    "\n",
    "There are 2 main methods to identify skewness in the data:\n",
    "- observation method: this method relies on plotting the histogram of data and and look for statistical indicators like mean, median and mode.\n",
    "For right-skewed data it holds that\n",
    "$$mean > median > mode$$\n",
    "while for left-skewed data it holds that\n",
    "$$mean < median < mode$$\n",
    "- statistical method: with this method we compute skewness as\n",
    "$$skewness=\\frac{3(mean - median)}{stddev}$$\n",
    "\n",
    "Most of the statistical models do not work when the data is skewed and this is because the tail region of the skewed data distributions are outliers in the data and they can severely damage the performance of a statistical model.  \n",
    "To deal with skewed data there are a bunch of techiques that are well represented by the ladder of powers that is\n",
    "- cube power\n",
    "- square power\n",
    "- square root\n",
    "- logarithm\n",
    "- shifted logarithm\n",
    "- reciprocal\n",
    "- reciprocal square power\n",
    "\n",
    "When data is right-skewed try moving down the ladder of powers to find the best transformation in order to remove skewness whereas if data is left-skewed try moving up the ladder of powers.\n",
    "\n",
    "**Note** that this transformations require data to be positive. Some of them as log transformation in particular require data to be strictly positive\n",
    "\n",
    "**References**\n",
    "- http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature skewness\n",
    "high_skew_th = .5\n",
    "feat_skew = X[num_cols].apply(lambda x: sp.stats.skew(x.dropna())).reset_index().rename({'index': 'feature', 0: 'skew'}, axis=1)\n",
    "high_skew_feat = feat_skew[(feat_skew['skew'] < -high_skew_th) | (feat_skew['skew'] > high_skew_th)]['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation ladder\n",
    "c = high_skew_feat[0]\n",
    "\n",
    "np.power(X[c], 3)\n",
    "np.power(X[c], 2)\n",
    "np.sqrt(X[c])                              # note that this transf has fixed point in 1\n",
    "np.log(X[c])                               # note that this transf gives error if data is 0\n",
    "np.log1p(X[c])                             # this one adds 1 to data in order to solve log issue\n",
    "np.reciprocal(X[c])\n",
    "np.power(X[c], -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed098669",
   "metadata": {},
   "source": [
    "Other transformation are **power transformations**. In statistics, power transform is a family of functions applied to create a monotonic transformation of data using power functions. It is a data transformation technique used to stabilize variance, make the data more normal distribution-like, improve the validity of measures of association (such as the Pearson correlation between variables), and for other data stabilization procedures.\n",
    "\n",
    "A very useful case of power transform is **Box-Cox transformation** that follows from the inclusion of the $(λ − 1)$-th power of the geometric mean in the denominator. This simplifies the scientific interpretation of the equation because the units of measurement do not change as $λ$ changes. Box-Cox transformation is defined as follow:\n",
    "\n",
    "$$\\hat{y}_t = \\begin{cases} log(y_{t}), & λ=0\\\\\\frac{(y_{t}^{λ}-1)}{λ}, & λ!=0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5482f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox transf\n",
    "# Box-Cox is available in scipy from scipy.stats.boxcox or scipy.special.boxcox\n",
    "# the only difference between the 2 is that scipy.stats.boxcox estimates lambda if you do not provide it to the formula\n",
    "# note that Box-Cox requires data to be strictly positive\n",
    "bc_data, lamb = sp.stats.boxcox(X[c])     \n",
    "bc_data = sp.special.boxcox(X[c])\n",
    "\n",
    "# if you have null data then you can use either of the following 2 approaches\n",
    "#  - use boxcox1p formula that applies the boxcox transformation to data plus 1\n",
    "lam = .15\n",
    "bc_data = sp.special.boxcox1p(X[c], lam)\n",
    "#  - you can transform separately positive and null values. By definition Box-Cox transformation of a null value is 1/lambda where lambda is estimated by the model on positive values\n",
    "pos_col = X[X[c] > 0][c]\n",
    "bc_data, lamb = sp.stats.boxcox(pos_col)\n",
    "bc_col = np.empty_like(X[c])\n",
    "bc_col[X[c] > 0] = bc_data\n",
    "bc_col[X[c] == 0] = -1 / lamb\n",
    "X[c] = bc_col\n",
    "\n",
    "# finally you can estimate the optimal Box-Cox transform parameter for input data with the following method\n",
    "sp.stats.boxcox_normmax(X[c], method='mle')       # default method is pearson that maximizes the pearson correlation but mle is actually the one used in sp.stats.boxcox\n",
    "sp.stats.boxcox_normmax(X[c] + 1, method='mle')   # add 1 if you are using boxcox1p\n",
    "\n",
    "# to get the Box-Cox inverse transform use the following\n",
    "sp.special.inv_boxcox(X[c], lam)\n",
    "sp.special.inv_boxcox1p(X[c], lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504a40b",
   "metadata": {},
   "source": [
    "<a name=\"sel\"></a>\n",
    "## Feature Selection\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Feature Selection is the process of selecting a subset of relevant features from a larger set of features in a dataset. The goal is to identify the most informative and discriminative features that contribute most to the predictive power of the model.\n",
    "\n",
    "Note that when doing feature reduction you are actually removing some information from your data and this might also affect the model performances.\n",
    "\n",
    "Feature selection techniques\n",
    "- **Filter methods**: they rank features based on statistical metrics or heuristic measures. They assess the relevance of each feature independently of the learning algorithm. Popular methods are:\n",
    "    - **Correlation-based Feature Selection (CFS)**: evaluates the correlation between features and the target variable\n",
    "    - **Information Gain (IG)**: measures the reduction in entropy or impurity after including a particular feature\n",
    "    - **Min-Redundancy Max-Relevance (MRMR)**: it works iteratively, selecting one feature at a time. At each step, it calculates the feature with the maximum score among the remaining unselected features. Note that it only works for supervised learning tasks\n",
    "- **Wrapper methods**: they evaluate subsets of features by training and testing a specific ML model. They assess the performance of the model with different feature subsets to determine the optimal set of features. Common methods are:\n",
    "    - **Recursive Feature Elimination (RFE)**: starts with all features and recursively eliminates the least important ones\n",
    "    - **Genetic Algorithms (GA)**: uses an evolutionary algorithm to search for an optimal feature subset\n",
    "    - **Boruta**: it creates shadow features which are shuffled copies of input features and combines them with the original ones. Then it runs a random forest classifier on the combined dataset and compute the z-scores for the variables. Finally it computes the maximum z-score among shadow features (MZSA) and keeps features that have significantly higher importance than MZSA while discarding features with significantly lower importance than MZSA\n",
    "- **Embedded methods**: they incorporate feature selection within the model training process itself. The model automatically selects the most relevant features while learning the patterns in the data. Some examples are:\n",
    "    - **L1 regularization (Lasso)**: it penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant features to exactly 0, which removes those features from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d863f1a",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information gain refers to a family of techniques that calculate the reduction in entropy or surprise when transforming a given dataset in some way.\n",
    "\n",
    "When applied to feature selection it is referred to as **mutual information** as it measures dependency between 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = pd.Series(mutual_info_classif(X, y), index=X.columns)\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_features = 20\n",
    "\n",
    "st = time.time()\n",
    "mutual_info = SelectKBest(mutual_info_classif, k=5)\n",
    "mutual_info.fit(X, y)\n",
    "logging.info(f'Information Gain feature selection runtime: {timedelta(seconds=time.time() - st)}')\n",
    "mutual_info.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e595f",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE looks for a subset of features by starting with all features in the training dataset and iteratively discarding features until the desired number of features remains. This is achieved by fitting a ML algorithm to data, then ranking features by importance, discarding the least important features and re-fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18617946",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_features = 20\n",
    "\n",
    "st = time.time()\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=nr_features)\n",
    "rfe.fit(X, y)\n",
    "logging.info(f'RFE feature selection runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "X, y = rfe.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001962f",
   "metadata": {},
   "source": [
    "### Stepwise Regression\n",
    "\n",
    "Stepwise regression is the method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some criterion.  \n",
    "The primary advantage of stepwise regression is that it's **computationally efficient**. On the other side, its performances are generally worse than other methods. This is because, by making a hard selection on the next regressor and freezing the weight, it makes choices that are **locally optimal** at each step, but are **globally sub-optimal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700caeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_features = 20\n",
    "\n",
    "st = time.time()\n",
    "sfs = SequentialFeatureSelector(LogisticRegression(), k_features=nr_features, forward=True, scoring='accuracy', cv=None)\n",
    "selected_features = sfs.fit(X, y)\n",
    "logging.info(f'Stepwise regression feature selection runtime: {timedelta(seconds=time.time() - st)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd6fdf",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Lasso it's similar to stepwise regression but it has proven to yield better results. It penalizes the $L1$ norm of the weights, which induces sparsity in the solution. The degree of sparsity is controlled by the **penality term** that can be optimized through cross-validation or other optimization procedures.  \n",
    "See [Lasso](#lasso) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69202a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso feature selection\n",
    "lasso_model = make_pipeline(RobustScaler(), Lasso())\n",
    "\n",
    "search = GridSearchCV(\n",
    "    lasso_model,\n",
    "    {'model__alpha': np.arange(0.1,10,0.1)}, cv=5, scoring='neg_mean_squared_error', verbose=3\n",
    ")\n",
    "\n",
    "st = time.time()\n",
    "search.fit(X_train, y_train)\n",
    "logging.info(f'Lasso feature selection runtime: {timedelta(seconds=time.time() - st)}')\n",
    "logging.info(f'Lasso penalty factor: {search.best_params_}')\n",
    "\n",
    "# select features\n",
    "coefficients = search.best_estimator_.named_steps['model'].coef_\n",
    "importance = np.abs(coefficients)\n",
    "selected_features = np.array(X_train.columns)[importance > 0]\n",
    "discarded_features = np.array(X_train.columns)[importance == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0e7c9",
   "metadata": {},
   "source": [
    "### Boruta\n",
    "\n",
    "**Libraries**\n",
    "- https://www.rdocumentation.org/packages/Boruta/versions/7.0.0/topics/Boruta\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/simple-example-using-boruta-feature-selection-in-python-8b96925d5d7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8139a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 4\n",
    "forest = RandomForestRegressor(max_depth=5, n_jobs=n_jobs)\n",
    "boruta = BorutaPy(estimator=forest, n_estimators='auto', verbose=1, max_iter=100, random_state=123)\n",
    "\n",
    "print(f'Training Boruta on {len(non_iso_non_cat_cols)} features...')\n",
    "st = time.time()\n",
    "boruta.fit(np.array(X[non_iso_non_cat_cols]), np.array(y))\n",
    "print(f'Boruta feature selection runtime: {timedelta(seconds=time.time() - st)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ff4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check selected features\n",
    "boruta.support_\n",
    "\n",
    "# check ranking of features\n",
    "boruta.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide features according to their color\n",
    "green_area = list(compress(iso_walk_gross_cols, boruta_walk_gross.support_))\n",
    "blue_area = list(compress(iso_walk_gross_cols, boruta_walk_gross.support_weak_))\n",
    "red_area = [x for x in iso_walk_gross_cols if x not in green_area + blue_area]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cfc22",
   "metadata": {},
   "source": [
    "### Min-Redundancy Max-Relevance (MRMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif, mrmr_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_features = 20\n",
    "\n",
    "st = time.time()\n",
    "# for classification tasks\n",
    "selected_features = mrmr_classif(train_X[green_area+blue_area], train_y, K=nr_features, return_scores=True)\n",
    "# for regression tasks\n",
    "selected_features = mrmr_regression(train_X[green_area+blue_area], train_y, K=nr_features, return_scores=True)\n",
    "logging.info(f'MRMR feature selection runtime: {timedelta(seconds=time.time() - st)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17118b6",
   "metadata": {},
   "source": [
    "<a name=\"red\"></a>\n",
    "## Dimensionality Reduction\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Dimensionality reduction refers to techniques that transform a high-dimensional dataset into a lower-dimensional representation while preserving its essential structure and characteristics. The aim is to reduce the computational complexity, improve visualization, and eliminate redundant or noisy features.\n",
    "\n",
    "Dimensionality reduction techniques\n",
    "- **Principal Component Analysis (PCA)**: linear techinique that identifies a new set of orthogonal axes in the data, called principal components, which capture the maximum variance in the dataset. In PCA, the transformation is purely unsupervised, meaning that no information about the targets is used, therefore any highly predictive feature having low variance will be dropped\n",
    "- **Partial Least Squares (PLS)**: supervised method similar to PCA. PLS will try to find the hyperplane in the dependant data that explains the maximum variance in the target data the main difference with PCA thus being that the identified hyperplane is the most descriptive of the target data. \n",
    "- **Linear Discriminant Analysis (LDA)**: supervised method commonly used in classification tasks. It aims to maximize the separability between different classes by finding a projection that maximizes the between-class scatter and minimizes the within-class scatter \n",
    "- **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: non-linear technique particularly used for data visualization tasks. Its main characteristic is that it preserve the local structure of data while reducing the its dimension, meaning that data points that are close one to another in higher dimension space are represented close to each other in the low dimension space with high probability\n",
    "- **Uniform Manifold Approximation and Projection (UMAP)**: non-linear algorithm that aims to represend data in a lower dimension space while preserving distance between data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1eaba6",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_feat_red and feat_red_algo == 'PCA':\n",
    "    print(f'Performing PCA feature reduction...')\n",
    "    \n",
    "    pca_df = X_feat_scal.copy()\n",
    "\n",
    "    # identifying the best number of components for PCA\n",
    "    pca = PCA(svd_solver='full')\n",
    "    pca.fit(pca_df)\n",
    "\n",
    "    # explained variance\n",
    "    expl_var = pca.explained_variance_ratio_\n",
    "    cum_expl_var = np.cumsum(expl_var)\n",
    "\n",
    "    print(f'\\t75% of the variance can be attributed to {len([c for c in cum_expl_var if c <= .75])} features')\n",
    "    print(f'\\t80% of the variance can be attributed to {len([c for c in cum_expl_var if c <= .80])} features')\n",
    "    print(f'\\t85% of the variance can be attributed to {len([c for c in cum_expl_var if c <= .85])} features')\n",
    "    print(f'\\t90% of the variance can be attributed to {len([c for c in cum_expl_var if c <= .90])} features')\n",
    "    print(f'\\t95% of the variance can be attributed to {len([c for c in cum_expl_var if c <= .95])} features')\n",
    "    print(f'\\t99% of the variance can be attributed to {len([c for c in cum_expl_var if c <= .99])} features')\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(30,7))\n",
    "#     ax.plot(cum_expl_var, label='expl variance')\n",
    "#     ax.plot([.75]*len(expl_var), label='75%')\n",
    "#     ax.plot([.90]*len(expl_var), label='90%')\n",
    "#     ax.plot([.95]*len(expl_var), label='95%')\n",
    "#     ax.plot([.99]*len(expl_var), label='99%')\n",
    "#     ax.legend();\n",
    "    \n",
    "    num_feats = 102\n",
    "    pca = PCA(n_components=num_feats, svd_solver='full')\n",
    "    pca.fit(X_feat_scal)\n",
    "    pca_samples = pca.transform(X_feat_scal)\n",
    "\n",
    "    X_feat_red = pd.DataFrame(pca_samples)\n",
    "    print(f'Reduced to {len(X_feat_red)} samples and {len(X_feat_red.columns.tolist())} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43d8e2",
   "metadata": {},
   "source": [
    "### PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d72fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dimensions = 5\n",
    "pls = PLSRegression(n_components=n_dimensions)\n",
    "pls.fit(X[num_cols], y)\n",
    "X_red = pls.transform(X[num_cols])\n",
    "print(f'Reduced X dimension: {X_red.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3972a88",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "Note that LDA makes the following assumptions on data\n",
    "- each feature in the dataset follows a gaussian distribution\n",
    "- each feature has the same variance\n",
    "- lack of multicollinearity in independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a32730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ede96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "transformed = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78089c1e",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "**t-distributed Stochastic Neighbor Embedding (t-SNE)** is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.  \n",
    "It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n",
    "\n",
    "A data point is a point $x_i$ in the original data space $\\mathbf{R}^D$, where $D=64$ is the dimensionality of the data space. Every point is an image of a handwritten digit here. There are $N=1797$ points.\n",
    "\n",
    "A map point is a point $y_i$ in the map space $\\mathbf{R}^2$. This space will contain our final representation of the dataset. There is a bijection between the data points and the map points: every map point represents one of the original images.\n",
    "\n",
    "How do we choose the positions of the map points? We want to conserve the structure of the data. More specifically, if two data points are close together, we want the two corresponding map points to be close too. Let's $\\left| x_i - x_j \\right|$ be the Euclidean distance between two data points, and $\\left| y_i - y_j \\right|$ the distance between the map points. We first define a conditional similarity between the two data points:\n",
    "\n",
    "$$p_{j|i} = \\frac{\\exp\\left(-\\left| x_i - x_j\\right|^2 \\big/ 2\\sigma_i^2\\right)}{\\displaystyle\\sum_{k \\neq i} \\exp\\left(-\\left| x_i - x_k\\right|^2 \\big/ 2\\sigma_i^2\\right)}$$\n",
    "\n",
    "This measures how close $x_j$ is from $x_i$, considering a Gaussian distribution around $x_i$ with a given variance $\\sigma_i^2$. This variance is different for every point; it is chosen such that points in dense areas are given a smaller variance than points in sparse areas. The original paper details how this variance is computed exactly.\n",
    "\n",
    "Now, we define the similarity as a symmetrized version of the conditional similarity:\n",
    "\n",
    "$$p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.patheffects as PathEffects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33db49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_scatter(x, y):\n",
    "    # choose a color palette with seaborn\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    # create a scatter plot\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[y])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # position of each label.\n",
    "        xtext, ytext = np.median(x[y == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hand-written digits data\n",
    "digits = load_digits()\n",
    "\n",
    "# split images from labels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "digits_proj = TSNE(random_state=1234).fit_transform(X)\n",
    "\n",
    "tsne_scatter(digits_proj, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af5d58",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f20c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "X_umap = reducer.fit_transform(X)\n",
    "\n",
    "def plot_UMAP():\n",
    "    for i in range(10):\n",
    "        plt.scatter(X_umap[y==i, 0], X_umap[y==i, 1], label=i, s=1)\n",
    "    plt.xlabel('1st UMAP component')\n",
    "    plt.ylabel('2nd UMAP component')\n",
    "    plt.title('UMAP')\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid()\n",
    "\n",
    "plot_UMAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ec2b2",
   "metadata": {},
   "source": [
    "<a name=\"pipe\"></a>\n",
    "## Preprocessing Pipelines\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('one_hot_encoder', OneHotEncoder(), ['city']),\n",
    "    ('label_encoder', LabelEncoder()),\n",
    "    ('ordinal_encoder', OrdinalEncoder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('min_max_scaler', MinMaxScaler()),\n",
    "    ('quant_transf', QuantileTransformer(n_quantiles=100, output_distribution='normal')),\n",
    "    ('k_bins_discr', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')),\n",
    "    ('null_replacer_knn', KNNImputer(n_neighbors=int(s))),\n",
    "    ('vect', CountVectorizer(), 'title'),\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('trunc_svd', TruncatedSVD(n_components=7)),\n",
    "    ('random_forest_class', RandomForestClassifier())])\n",
    "    \n",
    "X_train_fitted = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228108ff",
   "metadata": {},
   "source": [
    "In order to train a Pipeline you first have to identify columns by data type.\n",
    "This can be done with the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc78d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T15:30:15.433952Z",
     "start_time": "2022-06-14T15:30:15.427987Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366b607",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"ml\"></a>\n",
    "# Machine Learning models\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "You should save every model you experiment with so that you can come back easily to any model you want.  \n",
    "Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well.\n",
    "This will allow you to easily compare scores across model types, and compare the types of errors they make.\n",
    "You can easily save Scikit-Learn models by using Python's `pickle` module or by using the `joblib` library, which is more efficient at serializing large NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1af68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:53:51.730021Z",
     "start_time": "2022-06-20T13:53:51.721022Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('median_house_value', axis=1)\n",
    "y = pd.DataFrame(df['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7a323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:53:51.885112Z",
     "start_time": "2022-06-20T13:53:51.870123Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting dataframes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, shuffle=True, random_state=123)\n",
    "\n",
    "# splitting indexes\n",
    "# in this case you can later train the model and make predictions in the following manner\n",
    "# model.fit(X.loc[ix_train,:], y.loc[iy_train,:])\n",
    "ix_train, ix_test = train_test_split(X.index, stratify=y, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cf9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "402e4d7b",
   "metadata": {},
   "source": [
    "<a name=\"mlmodels\"></a>\n",
    "## Models\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfebf2b4",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b906c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(\n",
    "    tol=.00001,\n",
    "    verbose=0,\n",
    "    random_state=123)\n",
    "\n",
    "st = time.time()\n",
    "logr_model.fit(X_train, y_train)\n",
    "logging.info(f'Logistic Regression model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = logr_model.predict(X_train)\n",
    "y_val_pred = logr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16176503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load ml model\n",
    "joblib.dump(lr_model, 'logistic_regression_model.pkl')\n",
    "model = joblib.load('logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95a57f",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear regression fits a linear model with coefficients $\\omega$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "Mathematically it solves a problem of the form\n",
    "\n",
    "$$\\min\\limits_{\\omega} \\left( \\left\\|\\omega X - y\\right\\|^2_2 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linr_model = LinearRegression()\n",
    "\n",
    "st = time.time()\n",
    "linr_model.fit(X_train, y_train)\n",
    "logging.info(f'Linear Regression model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = linr_model.predict(X_train)\n",
    "y_val_pred = linr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c77b1",
   "metadata": {},
   "source": [
    "<a name=\"lasso\"></a>\n",
    "### Lasso\n",
    "\n",
    "Lasso stands for **Least Absolute Shrinkage and Selection Operator** and it is a linear regression algorithm which performs $L1$ regularization to the coefficients meaning that it adds a penalty equal to the absolute value of the coefficients.  \n",
    "The optimization objective for Lasso is:\n",
    "\n",
    "$$\\min\\limits_{\\omega} \\left( \\frac{1}{2n} \\left\\|\\omega X - y\\right\\|^2_2 + \\alpha\\left\\|\\omega\\right\\|_1 \\right)$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$\\min\\limits_{\\omega} \\left(\\sum_{i=1}^{n} \\left(y_i - \\sum_{j=1}^{p} x_{ij}\\beta_{j}\\right)^{2} + \\alpha \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| \\right)$$\n",
    "\n",
    "where $x$ is the train data, $y$ is the target data and $\\beta$ are the linear regression coefficients.  \n",
    "$n$ is the sample size and $p$ is the number of features for each train observation.\n",
    "\n",
    "Technically the Lasso model is optimizing the same objective function as the Elastic Net without $L2$ penalty.\n",
    "\n",
    "The Lasso procedure encourages simple, sparse models (ie models with fewer parameters). For this reason Lasso can also be used for variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.  \n",
    "Moreover Lasso is well-suited for models showing high levels of multicollinearity.\n",
    "\n",
    "**Note** that Lasso is very sensitive to outliers so it is recommended to use a robust scaling algorithm prior to Lasso training for example sklearn RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75265b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "lasso_model = Lasso(alpha=0.0005)\n",
    "\n",
    "st = time.time()\n",
    "lasso_model.fit(X_train, y_train)\n",
    "logging.info(f'Lasso model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = lasso_model.predict(X_train)\n",
    "y_val_pred = lasso_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6615f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust scaling\n",
    "# a simple method to automatically scaling features in a robust manner is using sklearn make_pipeline and RobustScaler\n",
    "lasso_model = make_pipeline(RobustScaler(), Lasso(alpha=0.0005))\n",
    "\n",
    "st = time.time()\n",
    "lasso_model.fit(X_train, y_train)\n",
    "logging.info(f'Lasso model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = lasso_model.predict(X_train)\n",
    "y_val_pred = lasso_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af847e8",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "Ridge regression is a linear least squares algorithm with $L2$ regularization on the coefficients.  \n",
    "The optimization objective for Ridge is:\n",
    "\n",
    "$$\\min\\limits_{\\omega} \\left( \\left\\|\\omega X - y\\right\\|^2_2 + \\alpha\\left\\|\\omega\\right\\|^2_2 \\right)$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$\\min\\limits_{\\omega}\\left(\\sum_{i=1}^{n} \\left(y_i - \\sum_{j=1}^{p} x_{ij}\\omega_{j}\\right)^{2} + \\alpha \\sum_{j=1}^{p}\\left(\\omega_{j}\\right)^{2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "ridge_model = Ridge(alpha=.001)\n",
    "\n",
    "st = time.time()\n",
    "ridge_model.fit(X_train, y_train)\n",
    "logging.info(f'Ridge model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = ridge_model.predict(X_train)\n",
    "y_val_pred = ridge_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481405c",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "\n",
    "Kernel Ridge regression (**KRR**) combines Ridge regression with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c155bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "krr_model = KernelRidge(alpha=.001, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "st = time.time()\n",
    "krr_model.fit(X_train, y_train)\n",
    "logging.info(f'Kernel Ridge model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = krr_model.predict(X_train)\n",
    "y_val_pred = krr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e0bb4",
   "metadata": {},
   "source": [
    "### Bayesian Ridge\n",
    "\n",
    "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure in such a way that the regularization parameter is not set in a hard sense but tuned to the data at hand.\n",
    "\n",
    "This can be done by introducing uninformative priors over the hyperparameters of the model. The $L2$ regularization used in Ridge regression and classification is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients $\\omega$ with precision $\\lambda^{-1}$.\n",
    "Instead of setting $\\lambda$ manually, it is possible to treat it as a random variable to be estimated from the data.\n",
    "\n",
    "To obtain a fully probabilistic model, the output $y$ is assumed to be Gaussian distributed around $\\omega X$\n",
    "\n",
    "$$p\\left(y|X,\\omega,\\alpha \\right) = \\mathcal{N}\\left(y|\\omega X,\\alpha \\right)$$\n",
    "\n",
    "where $\\alpha$ is again treated as a random variable that is to be estimated from the data.\n",
    "\n",
    "The advantages of Bayesian Regression are:\n",
    "- it adapts to the data at hand\n",
    "- it can be used to include regularization parameters in the estimation procedure\n",
    "\n",
    "whereas the disadvantages of Bayesian regression include:\n",
    "- inference of the model can be time consuming.\n",
    "\n",
    "Bayesian Ridge regression estimates a probabilistic model of the regression problem as described above.\n",
    "The prior for the coefficient $\\omega$ is given by a spherical Gaussian\n",
    "\n",
    "$$p\\left(\\omega|\\lambda \\right) = \\mathcal{N}\\left(\\omega|0,\\lambda^{-1}I_p \\right)$$\n",
    "\n",
    "The priors over $\\alpha$ and $\\lambda$ are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "brr_model = BayesianRidge(alpha=.001)\n",
    "\n",
    "st = time.time()\n",
    "brr_model.fit(X_train, y_train)\n",
    "logging.info(f'Bayesian Ridge model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = brr_model.predict(X_train)\n",
    "y_val_pred = brr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a1973",
   "metadata": {},
   "source": [
    "### Automatic Relevance Determination\n",
    "\n",
    "Automatic Relevance Determination (**ARD**) regression is very similar to Bayesian Ridge Regression, but can lead to sparser coefficients $\\omega$.\n",
    "\n",
    "It poses a different prior over $\\omega$, by dropping the assumption of the Gaussian being spherical.\n",
    "Instead, the distribution over $\\omega$ is assumed to be an axis-parallel, elliptical Gaussian distribution.\n",
    "This means each coefficient $\\omega_i$ is drawn from a Gaussian distribution, centered on zero and with a precision $\\lambda_i$\n",
    "\n",
    "$$p\\left(\\omega|\\lambda \\right) = \\mathcal{N}\\left(\\omega|0,A^{-1} \\right)$$\n",
    "\n",
    "with $diag(A) = \\lambda = \\{\\lambda_1, ..., \\lambda_p\\}$.\n",
    "\n",
    "In contrast to Bayesian Ridge regression, each coordinate of $\\omega_i$ has its own standard deviation $\\lambda_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ba539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "ard_model = ARDRegression(alpha=.001, n_iter=1000, tol=1e-4)\n",
    "\n",
    "st = time.time()\n",
    "ard_model.fit(X_train, y_train)\n",
    "logging.info(f'ARD model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = ard_model.predict(X_train)\n",
    "y_val_pred = ard_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e55a5",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "\n",
    "The ElasticNet is a regularized regression method that linearly combines the $L1$ and $L2$ penalties of the Lasso and Ridge methods.    \n",
    "The optimization objective for ElasticNet is:\n",
    "\n",
    "$$\\min\\limits_{\\omega} \\left( \\frac{1}{2n}\\left\\|\\omega X - y\\right\\|^2_2 + \\alpha\\rho\\left\\|\\omega\\right\\|_1 + \\frac{1}{2}\\alpha(1-\\rho)\\left\\|\\omega\\right\\|^2_2 \\right)$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$\\min\\limits_{\\omega} \\left( \\frac{1}{2n} \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=1}^{p} x_{ij}\\omega_{j} \\right)^{2} + \\alpha \\rho \\sum_{j=1}^{p}\\left|\\omega_{j} \\right| + \\frac{1}{2} \\alpha (1 - \\rho) \\sum_{j=1}^{p}\\left(\\omega_{j}\\right)^{2} \\right)$$\n",
    "\n",
    "where $x$ is the train data, $y$ is the target data and $\\beta$ are the linear regression coefficients.  \n",
    "$n$ is the sample size and $p$ is the number of features for each train observation.  \n",
    "$\\rho$ is called the $l1$ ratio and it is the ElasticNet mixing parameter. It ranges between 0 and 1 included with the limit cases being Lasso when it is 1 and a pure $L2$ penalty when it is 0.\n",
    "\n",
    "**Note** that similarly to Lasso, ElasticNet is very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "enet_model = ElasticNet(alpha=0.0005, l1_ratio=.9)\n",
    "\n",
    "st = time.time()\n",
    "enet_model.fit(X_train, y_train)\n",
    "logging.info(f'Elastic Net model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = enet_model.predict(X_train)\n",
    "y_val_pred = enet_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609d0f2",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes refers to a family of classification algorithms based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "\n",
    "Bayes' theorem states that, given a class variable $y$ and dependent feature vector $X$:\n",
    "\n",
    "$$P(y|X) = \\frac{P(y)P(X|y)}{P(X)}$$\n",
    "\n",
    "Using the naive conditional independence assumption, meaning that\n",
    "\n",
    "$$P(x_i|y,x_1,...x_{n-1},x_{n+1},...,x_n) = P(x_i|y)$$\n",
    "\n",
    "for all $i$, Bayes relationship is simplified to\n",
    "\n",
    "$$P(y|X) = \\frac{P(y)\\prod_{i=1}^nP(x_i|y)}{P(X)}$$\n",
    "\n",
    "Now, since $P(X)$ is constant given the input, we can derive the following\n",
    "\n",
    "$$P(y|X)\\propto P(y)\\prod_{i=1}^nP(x_i|y)$$\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "$$\\hat y = arg\\max\\limits_y P(y)\\prod_{i=1}^nP(x_i|y)$$\n",
    "\n",
    "and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i|y)$, the former being the relative frequency of class $y$ in the training set.\n",
    "\n",
    "The different Naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5005e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian naive bayes\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "st = time.time()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "logging.info(f'Gaussian Naive Bayes model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = gnb_model.predict(X_train)\n",
    "y_val_pred = gnb_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1f697",
   "metadata": {},
   "source": [
    "### Support Vectore Machine\n",
    "\n",
    "A Support Vector Machine (**SVM**) ...\n",
    "\n",
    "The objective of the SVM algorithm is to find the **maximimum-margin hyperplane** in a $p$-dimensional space where $p$ is the dimension of the data. Maximum-margin refers to the request that the hyperplane is defined so that the distance between the minimum data points in the 2 semiplanes is maximized.\n",
    "\n",
    "In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the **kernel trick**, implicitly mapping their inputs into high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3eb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "    kernel='rbf',\n",
    "    degree=3,\n",
    "    verbose=0\n",
    "}\n",
    "\n",
    "# classification\n",
    "svc_model = SVC(**svm_params)\n",
    "\n",
    "# regression\n",
    "svr_model = SVR(**svm_params)\n",
    "\n",
    "st = time.time()\n",
    "svr_model.fit(X_train, y_train)\n",
    "logging.info(f'SVR model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = svr_model.predict(X_train)\n",
    "y_val_pred = svr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6193e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14386253",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {\n",
    "    max_depth:3,\n",
    "    min_samples_split:3,\n",
    "    random_state:123\n",
    "}\n",
    "\n",
    "# classification\n",
    "tree_model = DecisionTreeClassifier(**tree_params)\n",
    "\n",
    "# regression\n",
    "tree_model = DecisionTreeRegressor(**tree_params)\n",
    "\n",
    "st = time.time()\n",
    "tree_model.fit(X_train, y_train)\n",
    "logging.info(f'Decision Tree model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = tree_model.predict(X_train)\n",
    "y_val_pred = tree_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tree plot\n",
    "plt.figure(figsize=(40, 10))\n",
    "plot_tree(tree_model, feature_names=X_train.columns.tolist(), filled=True, fontsize=12)\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.savefig(f'tree_decision_split.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331d60b",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning algorithm based on decision trees. The ensemble model is built with **bagging** technique by **boostrapping the data** by randomly choosing subsamples of the train data and training a decision tree on each. Decision trees are trained in parallel and the final prediction is made by averaging the predicitons made by all the decision trees. \n",
    "\n",
    "**Note** that Random Forest is based on trees whose leaves contain only constant values. This means that a given random forest model or any other tree based model can only predict a discrete number of values. This number of values is called **cardinality** of the model.  \n",
    "Cardinality is fundamental for the following reasons:\n",
    "- if the model cardinality is much lower that the cardinality of the prediction set then the model is unable to capture the underlying data trend correctly. T\n",
    "he set of predictions is not large enough to capture the complexity of data;\n",
    "- if the model cardinality is much higher than the cardinality of the prediction set then the model is likely overfitting.\n",
    "\n",
    "**Note** that tree-based ensemble algorithms are **non-parametric models**. This means that they do NOT assume or require data to follow a specific distribution and therefore they do not features to be independent (allowing for multicollinearity in data), they do not require data to be scaled and they are usually robust to outliers and to overfitting.\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/the-ultimate-guide-to-adaboost-random-forests-and-xgboost-7f9327061c4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cadfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    n_estimators:700,\n",
    "    max_depth:7,\n",
    "    verbose:0,\n",
    "    random_state:123\n",
    "}\n",
    "\n",
    "# classification\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "# regression\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "\n",
    "st = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "logging.info(f'Random Forest model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_val_pred = rf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aecf9f7",
   "metadata": {},
   "source": [
    "To get the depth of trees in a random forest use one of the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set([estimator.tree_.max_depth for estimator in rf_model.estimators_]))\n",
    "list(set([estimator.get_depth() for estimator in rf_model.estimators_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bdb23",
   "metadata": {},
   "source": [
    "<a name=\"adaboost\"></a>\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost stands for **Adaptive Boosting** and it is an ensemble learning algorithm based on decision trees.  \n",
    "Differently from Random Forest, the ensemble model is built with boosting technique by sequentially growing decision trees as weak learners and punishing the incorrectly predicted samples by assigning a larger weight to them after each round of prediction. This way, the algorithm is learning from previous mistakes.  \n",
    "The final prediction is made by averaging the trees prediction.\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. \n",
    "\n",
    "**References**\n",
    "- https://medium.com/@divyagera2402/boosting-algorithms-adaboost-gradient-boosting-xgb-light-gbm-and-catboost-e7d2dbc4e4ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "ada_params = {\n",
    "    'n_estimators': 400,\n",
    "    'learning_rate' : 0.65\n",
    "}\n",
    "ada_model = AdaBoostClassifier(**ada_params)\n",
    "\n",
    "st = time.time()\n",
    "ada_model.fit(X_train, y_train)\n",
    "logging.info(f'AdaBoost model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = ada_model.predict(X_train)\n",
    "y_val_pred = ada_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427875",
   "metadata": {},
   "source": [
    "<a name=\"gboost\"></a>\n",
    "### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is similar to AdaBoost as it is a tree-based ensemble learning algorithm.  \n",
    "The main difference with [AdaBoost](#adaboost) lies in what Gradient Boosting does with the underfitted values of its predecessor. Contrary to AdaBoost, which tweaks the instance weights at every interaction, this method tries to fit the new predictor to the residual errors made by the previous predictor.  \n",
    "\n",
    "**Note** that in regression task the default loss function is `squared_error` but `quantile` can be used to estimate prediction intervals (see [Prediction intervals](#regint) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbce902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b33685",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    n_estimators:500,\n",
    "    max_depth:5,\n",
    "    learning_rate:0.01,\n",
    "    min_samples_leaf:5,\n",
    "    min_samples_split:5,\n",
    "    random_state:12\n",
    "}\n",
    "\n",
    "# classification\n",
    "gb_model = GradientBoostingClassifier(**bg_params)\n",
    "\n",
    "# regression\n",
    "gb_model = GradientBoostingRegressor(**bg_params)\n",
    "\n",
    "st = time.time()\n",
    "gb_model.fit(X_train, y_train)\n",
    "logging.info(f'Gradient Boosting model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = gb_model.predict(X_train)\n",
    "y_val_pred = gb_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f6229",
   "metadata": {},
   "source": [
    "<a name=\"xgboost\"></a>\n",
    "### XGBoost\n",
    "\n",
    "XGBoost stands for **eXtreme Gradient Boosting** and it is an advanced implementation of [Gradient Boosting](#gboost).  \n",
    "Extreme refers to the fact that this implementation of Gradient Boosting algorithm is designed to \"*push the extreme of the computation limits of machines to provide a scalable , portable and accurate library.*”\n",
    "\n",
    "XGBoost grows **asymmetric trees** meaning that splitting condition for each node across the same depth can differ. Trees are grown **level-wise** (vertically) meaning that at each step all leaf nodes are exploded and evaluated. Splitting method ... XGBoost does not support categorical features, does not support text features and can handle missing values.\n",
    "\n",
    "XGBoost is among the best algorithms when dealing with classification problems.  \n",
    "On the other hand, XGBoost is not recommended when dealing with regression tasks that involve predicting a continuous variable, in tasks involving extrapolation or tasks where the target variable has values that are not present in the training data. This is because XGBoost is not able to predict values that it hasn't seen in training data.\n",
    "\n",
    "**Note** that in classification, XGBoost performances scale quadratically with the number of classes.\n",
    "\n",
    "**Libraries**\n",
    "- https://xgboost.readthedocs.io/en/stable/\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33773e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    n_estimators:700,\n",
    "    max_depth:7,\n",
    "    learning_rate:0.01,\n",
    "    gamma:0.1,\n",
    "    verbosity:0,\n",
    "    random_state:123\n",
    "}\n",
    "\n",
    "# regression\n",
    "xgb_model = XGBRegressor(xgb_params)\n",
    "\n",
    "st = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "logging.info(f'XGBoost model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_val_pred = xgb_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316ea78",
   "metadata": {},
   "source": [
    "<a name=\"lgbm\"></a>\n",
    "### LightGBM\n",
    "\n",
    "LightGBM stands for **Light Gradient Boosting Machine** and it's a [Gradient Boosting](#gboost) framework that uses tree based learning algorithms designed to be distributed and efficient allowing for faster parallel train, lower memory usage and better accuracy.\n",
    "\n",
    "Similarly to [XGBoost](#xgboost), it grows **asymmetric trees** but here instead trees are grown **leaf-wise** (horizontally) resulting in smaller and faster models compared to XGBoost.\n",
    "\n",
    "\n",
    "**Libraries**\n",
    "- https://lightgbm.readthedocs.io/en/v3.3.2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e33e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    n_estimators:720,\n",
    "    num_leaves:5,\n",
    "    learning_rate:0.05,\n",
    "    max_bin:55,\n",
    "    bagging_fraction:0.8,\n",
    "    bagging_freq:5,\n",
    "    min_data_in_leaf:6,\n",
    "    min_sum_hessian_in_leaf:11\n",
    "}\n",
    "\n",
    "# regression\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', **lgb_params)\n",
    "\n",
    "st = time.time()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "logging.info(f'LightGBM model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = lgb_model.predict(X_train)\n",
    "y_val_pred = lgb_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ee52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature importance\n",
    "for col in cat_cols:\n",
    "    X[col] = pd.factorize(X[col])[0]\n",
    "    X[col] = X[col].astype('category')\n",
    "    \n",
    "# lgbm\n",
    "train_set = lgb.Dataset(X[cat_cols], label=y)\n",
    "params = {'n_estimators':100, 'max_depth':5, 'learning_rate':0.1, 'num_leaves':500}\n",
    "lgbm = lgb.train(params=params, train_set=train_set, categorical_feature=cat_cols)\n",
    "\n",
    "print('Plot feature importances...')\n",
    "plt.figure(figsize=(30,15))\n",
    "ax = plt.subplot(111)\n",
    "lgb.plot_importance(lgbm, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc94e3e",
   "metadata": {},
   "source": [
    "<a name=\"catboost\"></a>\n",
    "### CatBoost\n",
    "\n",
    "CatBoost is a more recent implementation of [Gradient Boosting](#gboost).\n",
    "\n",
    "It differs from [XGBoost](#xgboost) and [LightGBM](#lgbm) since it grows **symmetric trees** meaning that the splitting condition is consistent across all nodes at the same depth of the tree and therefore the splitting condition must result in the lowest loss across all nodes of the same depth. CatBoost supports numerical, categorical and text features and can handle missing values.\n",
    "\n",
    "`Quantile` loss function can be used to estimate prediction intervals (see [Prediction intervals](#regint) for details)\n",
    "\n",
    "**Note** that CatBoost train time can increase a lot with trees' depth.\n",
    "\n",
    "**Libraries**\n",
    "- https://catboost.ai/en/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "catb_params = {\n",
    "    n_estimators:700,\n",
    "    depth:7,\n",
    "    learning_rate:0.1,\n",
    "    l2_leaf_reg:0.5,\n",
    "    verbose:0,\n",
    "    random_state:123\n",
    "}\n",
    "\n",
    "# classification\n",
    "catb_model = CatBoostClassifier(**catb_params)\n",
    "\n",
    "# regression\n",
    "catb_model = CatBoostRegressor(**catb_params)\n",
    "\n",
    "st = time.time()\n",
    "catb_model.fit(X_train, y_train)\n",
    "logging.info(f'CatBoost model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = catb_model.predict(X_train)\n",
    "y_val_pred = catb_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c46776",
   "metadata": {},
   "source": [
    "Feature importance can be found thanks to the `get_feature_importance()` method.  \n",
    "The output feature importance depends on the specified `type`, specifically:\n",
    "- **PredictionValuesChange**: (default) for each feature it shows how much on average the prediction changes if the feature value changes, the bigger the value of the importance the bigger on average is the change to the prediction value, if this feature is changed;\n",
    "- **LossFunctionChange**: for each feature the value represents the difference between the loss value of the model with this feature and without it. Since it is computationally expensive to retrain the model without one of the features, this model is built approximately using the original model with this feature removed from all the trees in the ensemble. This type is particularly effective for **ranking models**;\n",
    "- **PredictionDiff**: for each feature it reflects the maximum possible change in the predictions difference if the value of the feature is changed for both objects. The change is considered only if there is an improvement in the direction of changing the order of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostRegressor(max_depth=5) \n",
    "catboost.fit(X, y, verbose=False)\n",
    "catboost_feat_imp = catboost.get_feature_importance()\n",
    "\n",
    "# create df to inspect\n",
    "catboost_features = pd.DataFrame(X.columns, columns=['feature'])\n",
    "catboost_features['importance'] = catboost_feat_imp\n",
    "catboost_features = catboost_features.sort_values('importance', ascending=False).reset_index().reset_index().rename({'index':'org_order', 'level_0':'importance_order'}, axis=1)\n",
    "catboost_features['cumulative_importance'] = np.cumsum(catboost_features['importance'])\n",
    "display(catboost_features[['feature','importance','cumulative_importance']].head())\n",
    "\n",
    "# take the features that account for 95% of importance\n",
    "n_features = catboost_features[catboost_features.cumulative_importance < 95].iloc[-1].importance_order + 1\n",
    "print(f'Selected {n_features} features')\n",
    "\n",
    "plt.figure(figsize=(40,7))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(catboost_features.importance_order, catboost_features.cumulative_importance, label='cumulative importance')\n",
    "ax.axhline(y=95, color='r', linestyle='-', label='95% importance')\n",
    "ax.axvline(x=n_features, color='r', linestyle='-', label='selected features')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5bc5b",
   "metadata": {},
   "source": [
    "Moreover, if you want to reduce the size of the train dataset to a given number of features you can also take advantage of the `select_features()` method\n",
    "\n",
    "This method iteratively discards features from the train set in order to reach the input number of features.  \n",
    "Finally it plots the loss by the eliminated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff078641",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostRegressor(max_depth=5) \n",
    "catboost_selected_feats = catboost.select_features(\n",
    "    X, y, features_for_select=X.columns, num_features_to_select=100, verbose=False, plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686184c",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "\n",
    "Ensemble models is a machine learning approach to combine multiple other models in the prediction process. Those models are referred to as base estimators.\n",
    "\n",
    "It is a solution to overcome the following technical challenges of building a single estimator:\n",
    "- high variance: the model is very sensitive to the provided inputs to the learned features;\n",
    "- low accuracy: one model or one algorithm to fit the entire training data might not be good enough to meet expectations;\n",
    "- features noise and bias: the model relies heavily on one or a few features while making a prediction.\n",
    "\n",
    "There are different ensemble techniques such as:\n",
    "- **bagging**: multiple models are trained seprately on different subsets of the training data. This approach reduces variance and minimizes overfitting.\n",
    "- **boosting**: in this technique multiple models are trained sequentially so that each new model is trained on the error produced by the previous model. This approach reduces bias.\n",
    "- **stacking**:\n",
    "- **blending**:\n",
    "\n",
    "#### Stacking\n",
    "\n",
    "The general framework of a stacked ensemble consists of two or more **base models** (level-0 models) and a higher level **meta model** (level-1 model) with their functions as below:\n",
    "- base models: models that fit the training data and predict out-of-sample data;\n",
    "- meta model: model that fits on the prediction from base-models and learns how to best combine the predictions.\n",
    "\n",
    "Stacking is appropriate when multiple different machine learning models have skill on a dataset, but have skill in different ways. Another way to say this is that the predictions made by the models or the errors in predictions made by the models are uncorrelated or have a low correlation.\n",
    "\n",
    "Base models are often complex and diverse. As such, it is often a good idea to use a range of models that make very different assumptions about how to solve the predictive modeling task, such as linear models, decision trees, support vector machines, neural networks, and more. Other ensemble algorithms may also be used as base-models, such as random forests.\n",
    "\n",
    "There hasn't been any research on how to choose or optimize the meta model in this case. In most cases, the meta-model used is just a simple model such as Linear Regression for regression tasks and Logistic Regression for classification tasks. One reason why more complex meta-models are often not chosen is because there is a much higher chance that the meta-model may overfit to the predictions from the base models.\n",
    "\n",
    "**References**\n",
    "- https://www.kaggle.com/code/daisukelab/optimizing-ensemble-weights-using-simple\n",
    "- https://towardsdatascience.com/stacked-ensembles-improving-model-performance-on-a-higher-level-99ffc4ea5523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93394ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked ensemble\n",
    "estimators = [\n",
    "    ('ridge', RidgeCV()),\n",
    "    ('lasso', LassoCV(random_state=42)),\n",
    "    ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))\n",
    "]\n",
    "\n",
    "stack_reg = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "\n",
    "st = time.time()\n",
    "stack_reg.fit(X_train, y_train)\n",
    "logging.info(f'Stacked ensemble model train runtime: {timedelta(seconds=time.time() - st)}')\n",
    "\n",
    "y_train_pred = stack_reg.predict(X_train)\n",
    "y_val_pred = stack_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d931df",
   "metadata": {},
   "source": [
    "<a name=\"mlint\"></a>\n",
    "## Prediction intervals\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc043e",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "**Libraries**\n",
    "- https://contrib.scikit-learn.org/forest-confidence-interval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b445005",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=700,\n",
    "    max_depth=7,\n",
    "    verbose=0,\n",
    "    random_state=123)\n",
    "\n",
    "st = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(35,10))\n",
    "ax = plt.subplot(111)\n",
    "ax.errorbar(y_val, rf_sup.predict(X_va), yerr=np.sqrt(rf_model), fmt='o')\n",
    "ax.plot(y_val, y_val, 'r')\n",
    "ax.xlabel('Actual')\n",
    "ax.ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf13cd",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "**References**\n",
    "- https://medium.com/walmartglobaltech/adding-prediction-intervals-to-tree-based-models-8ea53814a4b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "all_predictions = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5,\n",
    "    verbose=0,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "for alpha in [0.05, 0.10, 0.5, 0.90, 0.95]:\n",
    "    gb_model = GradientBoostingRegressor(loss='quantile', alpha=alpha, **common_params)\n",
    "    # all_models[\"q %1.2f\" % alpha] = gb_model.fit(X_train, y_train)\n",
    "    # all_models['q {:.2f}'.format(alpha)] = gb_model.fit(X_train, y_train)\n",
    "    all_models['q_%02.f' % (alpha * 100)] = gb_model.fit(X_train, y_train)\n",
    "    all_predictions['q_%02.f' % (alpha * 100)] = all_models['q_%02.f' % (alpha * 100)].predict(X_val)\n",
    "\n",
    "gb_mse_model = GradientBoostingRegressor(loss='squared_error', **common_params)\n",
    "all_models['mse'] = gb_mse_model.fit(X_train, y_train)\n",
    "all_predictions['mse'] = all_models['mse'].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befa2f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T08:09:35.588333Z",
     "start_time": "2022-06-15T08:09:35.566269Z"
    }
   },
   "outputs": [],
   "source": [
    "nr_points = 100\n",
    "xx = np.linspace(1, nr_points, nr_points)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 7))\n",
    "ax = plt.subplot(111)\n",
    "# plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
    "ax.plot(xx, y_val[:nr_points], 'b.', label='Observation')\n",
    "ax.plot(xx, all_predictions['mse'][:nr_points], 'r.', label='Predictions')\n",
    "ax.plot(xx, all_predictions['q_90'][:nr_points], 'k-')\n",
    "ax.plot(xx, all_predictions['q_10'][:nr_points], 'k-')\n",
    "plt.fill(\n",
    "    np.concatenate([xx, xx[::-1]]),\n",
    "    np.concatenate([all_predictions['q_90'][:nr_points], all_predictions['q_10'][:nr_points][::-1]]),\n",
    "    alpha=.2, fc='b', ec='None', label='80% prediction interval')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6821fc9",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "all_predictions = {}\n",
    "common_params = dict(\n",
    "    n_estimators=700,\n",
    "    depth=7,\n",
    "    learning_rate=0.01,\n",
    "    l2_leaf_reg=0.5,\n",
    "    verbose=0,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "for alpha in [0.05, 0.10, 0.5, 0.90, 0.95]:\n",
    "    catb_model = CatBoostRegressor(loss_function='Quantile:alpha={}'.format(alpha), **common_params)\n",
    "    all_models['q_%02.f' % (alpha * 100)] = catb_model.fit(X_train, y_train)\n",
    "    all_predictions['q_%02.f' % (alpha * 100)] = all_models['q_%02.f' % (alpha * 100)].predict(X_val)\n",
    "\n",
    "catb_mse_model = CatBoostRegressor(**common_params)\n",
    "all_models['mse'] = catb_mse_model.fit(X_train, y_train)\n",
    "all_predictions['mse'] = all_models['mse'].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_points = 100\n",
    "xx = np.linspace(1, nr_points, nr_points)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 7))\n",
    "ax = plt.subplot(111)\n",
    "# plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
    "ax.plot(xx, y_val[:nr_points], 'b.', label='Observation')\n",
    "ax.plot(xx, all_predictions['mse'][:nr_points], 'r.', label='Predictions')\n",
    "ax.plot(xx, all_predictions['q_90'][:nr_points], 'k-')\n",
    "ax.plot(xx, all_predictions['q_10'][:nr_points], 'k-')\n",
    "plt.fill(\n",
    "    np.concatenate([xx, xx[::-1]]),\n",
    "    np.concatenate([all_predictions['q_90'][:nr_points], all_predictions['q_10'][:nr_points][::-1]]),\n",
    "    alpha=.2, fc='b', ec='None', label='80% prediction interval')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc65c1",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"dl\"></a>\n",
    "# Deep Learning models\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d97c0",
   "metadata": {},
   "source": [
    "#### Feed-Forward Neural Networks\n",
    "\n",
    "**Note** that there are several rules of thumb methods for determining the correct number of neurons in a hidden layer such as:\n",
    "- it should be between the size of the input layer and the size of the output layer\n",
    "- it should be 2/3 or the size of input layer plus the size of the output layer\n",
    "- it should be less than twice the size of the input layer\n",
    "\n",
    "**References**\n",
    "- https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3\n",
    "- https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c530fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "# clear tf session\n",
    "K.clear_session()\n",
    "\n",
    "# initialize nn\n",
    "nn = Sequential()\n",
    "\n",
    "# add layers\n",
    "nn.add(Dense(128, activation='relu', input_dim=n_features))\n",
    "nn.add(Dropout(.7))\n",
    "nn.add(Dense(128, activation='relu'))\n",
    "nn.add(Dropout(.7))\n",
    "nn.add(Dense(30, activation='relu'))\n",
    "nn.add(Dropout(.7))\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn.compile(loss='mean_squared_error', optimizer=Adam(lr=.002), metrics=['accuracy'])\n",
    "perf = nn.fit(X_train, y_train, epochs=500, batch_size=64, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1f193",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"hyper\"></a>\n",
    "# Hyperparameters Tuning\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017435c",
   "metadata": {},
   "source": [
    "<a name=\"cv\"></a>\n",
    "## Cross Validation\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Cross Validation is a resampling technique that can be used to evaluate and select machine learning algorithms on a limited dataset.\n",
    "k-fold cross validation is a type of cross validation, where the training data is split into k-folds and (k-1) folds is used for training and k-th fold is used for validation of the model.\n",
    "\n",
    "Common scoring metrics for **regression** are \n",
    "- neg_mean_absolute_error\n",
    "- neg_mean_squared_error\n",
    "- neg_root_mean_squared_error\n",
    "- neg_mean_squared_log_error\n",
    "- r2\n",
    "\n",
    "Common scoring metrics for **classification** are \n",
    "- accuracy\n",
    "- f1\n",
    "- precision\n",
    "- recall\n",
    "- roc_auc\n",
    "\n",
    "To inspect sklearn scoring metrics run `sklearn.metrics.SCORERS.keys()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model, X, y, cv=3, scoring=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3bada",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validatoin\n",
    "\n",
    "This method allows to try out all possible combinations of hyperparameters given a range of values for each of them.\n",
    "It uses cross validation to evaluate the performances of the model for a given combination of hyperparameters and then return the hyperparameters that are scoring the best result.\n",
    "\n",
    "You can run a Grid Search Cross Validation with the `GridSearchCV` method.\n",
    "This method will compute a model for every hyperparameter combination and each model will be trained n times where n is specified by the `cv` parameter.\n",
    "Hyperparameter ranges have to be provided to the method as a dictionary.\n",
    "You can also provide a dictionary of dictionaries in order to tell the Grid Search method to inspect the different hyperparameter combinations separately.\n",
    "\n",
    "**TIP**: when you have no idea what value a hyperparameter should have, a simple approach is to try out consecutive powers of 10\n",
    "\n",
    "**NOTE** that you can treat some of the data preparation steps as hyperparameters and that `GridSearchCV` can also be used to evaluate whether or not to add a feature, for outliers handling, for missing values imputation and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34caba9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T14:16:45.356797Z",
     "start_time": "2022-06-20T14:16:35.984876Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iter_search = 10\n",
    "param_distr = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3,4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(verbose=0, random_state=123),\n",
    "    param_grid=param_distr,\n",
    "    n_jobs=1,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    scoring='neg_mean_absolute_error')  # neg_mean_squared_error\n",
    "\n",
    "st = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "logging.info(f'GridSearchCV runtime: {timedelta(seconds=time.time() - st)} per {n_iter_search} setting dei parametri')\n",
    "logging.info(f'GridSearchCV best score: {grid_search.best_score_}')\n",
    "logging.info(f'GridSearchCV best parameters: {grid_search.best_params_}')\n",
    "logging.info(f'GridSearchCV results: {grid_search.cv_results_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123776a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T14:16:48.006013Z",
     "start_time": "2022-06-20T14:16:47.988083Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'GridSearchCV runtime: {timedelta(seconds=time.time() - st)} per {n_iter_search} setting dei parametri')\n",
    "print(f'GridSearchCV best estimator: {grid_search.best_estimator_}')   # this method returns a trained model\n",
    "print(f'GridSearchCV best parameters: {grid_search.best_params_}')\n",
    "print(f'GridSearchCV best score: {grid_search.best_score_}')\n",
    "print(f'GridSearchCV results: {grid_search.cv_results_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721f6f7",
   "metadata": {},
   "source": [
    "### Halving Grid Search Cross Validation\n",
    "\n",
    "This method is similar to Grid Search Cross Validation but it follows a successive halving approach in the following steps:\n",
    "- it trains a subset of data to all the combinations of parameters;\n",
    "- top performing candidates or combinations are selected;\n",
    "- a larger subset of training data is trained on the top-performing candidates\n",
    "- the above 3 steps are repeated until the best set of hyperparameters are left standing.\n",
    "\n",
    "Using Halving Grid Search, for each passing iteration, the parameter components are decreasing and the training dataset is increasing.\n",
    "Since the algorithm follows a successive halving approach, so the time complexity of the algorithm is comparatively very less compared to Grid Search Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad83776",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 4, 7, 10, 25],\n",
    "    'gamma': [0.5, 1, 5, 10, 25],\n",
    "    'min_child_weight': [1, 3, 5, 10, 25],\n",
    "    'reg_lambda': [5, 10, 50, 100, 300],\n",
    "    'scale_pos_weight': [1, 3, 5, 10, 25]\n",
    "}\n",
    "\n",
    "halving_search = HalvingGridSearchCV(\n",
    "    xgb.XGBClassifier(objective=\"binary:logistic\"), param_grid, scoring=\"roc_auc\", n_jobs=-1, min_resources=\"exhaust\", factor=3)\n",
    "halving_search.fit(X_train, y_train)\n",
    "logging.info(f'HalvingGridSearchCV runtime: {timedelta(seconds=time.time() - st)} per {n_iter_search} setting dei parametri')\n",
    "logging.info('HalvingGridSearchCV best parameters: ', halving_cv.best_params_)\n",
    "logging.info('HalvingGridSearchCV best score: ', halving_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac918d",
   "metadata": {},
   "source": [
    "### Random Search Cross Validation\n",
    "\n",
    "This method is useful when dealing with large hyperparameter search space since it evalueates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.  \n",
    "\n",
    "Benefits:\n",
    "- by simply setting the number of iterations, you have more control over the computing budget you want to allocate to hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a7d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T11:18:52.055073Z",
     "start_time": "2022-06-15T11:18:48.073972Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iter_search = 10\n",
    "param_distr = {\n",
    "    'n_estimators': [400, 600, 800, 1000, 1200],\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.001, 0.005, 0.01],\n",
    "    'gamma': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    XGBRegressor(verbosity=0, random_state=123),\n",
    "    param_distributions=param_distr,\n",
    "    n_iter=n_iter_search,\n",
    "    n_jobs=2,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    scoring='neg_mean_squared_error')\n",
    "\n",
    "st = time.time()\n",
    "rnd_search.fit(X_train, y_train)\n",
    "logging.info(f'RandomizedSearchCV runtime: {timedelta(seconds=time.time() - st)} per {n_iter_search} setting dei parametri')\n",
    "logging.info('RandomizedSearchCV best score: ', rnd_search.best_score_)\n",
    "logging.info('RandomizedSearchCV best parameters: ', rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec0678",
   "metadata": {},
   "source": [
    "<a name=\"optuna\"></a>\n",
    "## Optuna\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Optuna is an open source hyperparameter optimization framework.  \n",
    "It uses a history record of trials to determine which hyperparameter values to try next.\n",
    "Using this data, it estimates a promising area and tries values in that area.  \n",
    "Optuna then estimates an even more promising region based on the new result.\n",
    "It repeats this process using the history data of trials completed thus far.\n",
    "Specifically, it employs a **Bayesian optimization algorithm** called **Tree-structured Parzen Estimator**.\n",
    "\n",
    "To use Optuna you first need to define the objective function for Optuna to maximize.\n",
    "The objective function should take a `Trial` object as the input and return the score, a float value or a list of float values.\n",
    "\n",
    "The next step is to use the objective function to create a `Study` object and then optimize it.\n",
    "\n",
    "**Note** that\n",
    "- you can optimize whatever metric you like  \n",
    "- you can also run multi-objective optimization jobs simply by specifying multiple output metrics to the objective function and then you have to specify the optimization direction for each of them\n",
    "\n",
    "**Libraries**\n",
    "- https://optuna.readthedocs.io/en/stable/\n",
    "\n",
    "**References**\n",
    "- https://www.kaggle.com/code/hamzaghanmi/xgboost-catboost-using-optuna?scriptVersionId=94510532\n",
    "- https://optuna.readthedocs.io/en/latest/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f9ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T13:43:15.518093Z",
     "start_time": "2022-09-13T13:43:13.387648Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial, train_features, train_labels):\n",
    "    # use the trial obj to suggest a range and a type for each hyperparameter\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 250)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 5, 20)\n",
    "    # # you can also suggest float parameters with one of the following commands\n",
    "    # uniform_param = trials.suggest_uniform()\n",
    "    # loguiniform_param = trials.suggest_loguniform()\n",
    "    # discrete_uniform_param = trials.suggest_discrete_uniform()\n",
    "    \n",
    "    # define the model with the suggested ranges and types for each hyperparameter to be optimized by optuna\n",
    "    model = RandomForestClassifier(\n",
    "        criterion=criterion,\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_depth=max_depth,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    \n",
    "    # split data in order to train and test the model\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        test_size=0.25,\n",
    "        random_state=123,\n",
    "        stratify=train_labels,\n",
    "    )\n",
    "    model.fit(train_x, train_y)\n",
    "    y_pred = model.predict(valid_x)\n",
    "    \n",
    "    # otherwise you can consider using cross validation as follows\n",
    "    # scores = cross_val_score(model, x, y, cv=KFold(n_splits=10, shuffle=True, random_state=123), scoring='neg_root_mean_squared_error')\n",
    "    # return scores.mean()\n",
    "    \n",
    "    # add user defined attributes to the study\n",
    "    # these attributes will appear with the \"user_attrs_\" prefix in the study\n",
    "    trial.set_user_attr(\"class_report_validation\", classification_report(valid_y, y_pred, output_dict=True))\n",
    "    trial.set_user_attr(\"class_report_training\", classification_report(train_y, model.predict(train_x), output_dict=True))\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "    trial.set_user_attr(\"name\", 'RandForest')\n",
    "    trial.set_user_attr(\"id\", trial.number)\n",
    "    \n",
    "    # the output of the objective function has to me a number that optuna will optimize\n",
    "    # you can pick whatever metric, score or value\n",
    "    return f1_score(valid_y, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67daa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = partial(objective, train_features=X, train_labels=y)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(obj, n_trials=20, n_jobs=4)\n",
    "study_df = study.trials_dataframe().sort_values(by='value', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e457f71",
   "metadata": {},
   "source": [
    "`plot_optimization_history` shows the scores from all trials as well as the best score so far at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc80623",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f677b9",
   "metadata": {},
   "source": [
    "`plot_parallel_coordinate` interactively visualizes the hyperparameters and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a8e68",
   "metadata": {},
   "source": [
    "`plot_slice` shows the evolution of the search.\n",
    "You can see where in the hyperparameter space your search went and which parts of the space were explored more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f33a79",
   "metadata": {},
   "source": [
    "## Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0dfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49875748",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "These inclue sklearn `RidgeCV`, `LassoCV`, and `ElasticNetCV` and Ray `Tune` (https://docs.ray.io/en/latest/tune/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086b335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bda63ea9",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"perf\"></a>\n",
    "# Performances\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "**Remember** to always inspect feature importance as you might have forgot a relevant feature that had to be discarded as the target variable or some feature derived from it.\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/avoid-r-squared-to-judge-regression-model-performance-5c2bc53c8e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11074e9f",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmse_cv(model, X_train.values, y_train)\n",
    "print(f'Model score: {score.mean():.4f} ({score.std():.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importances(xgb_model, X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba641ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN PERFORMANCES')\n",
    "regression_performances(y_train, y_train_pred, metrics=['mae', 'rmse'])\n",
    "print('TEST PERFORMANCES')\n",
    "regression_performances(y_val, y_val_pred, metrics=['mae', 'rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9cf43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T16:11:15.174681Z",
     "start_time": "2022-07-22T16:11:12.755916Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_regression_errors(xgb_model, X_train, X_val, y_train, y_val, target_='fatturato')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e4420",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a37c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CLASSIFICATION PERFORMANCES')\n",
    "classification_performances(y, y_pred)\n",
    "                            \n",
    "# to get performances in pandas DataFrame\n",
    "perf = pd.Series(classification_performances(y, y_pred, verbose=0)).to_frame().T\n",
    "display(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot performances\n",
    "plot_classification_performances(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18f02a",
   "metadata": {},
   "source": [
    "<a name=\"shap\"></a>\n",
    "## SHAP\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "SHAP stands for **SHapley Additive exPlanations**.\n",
    "\n",
    "SHAP values are based on **Shapley values**, a concept coming from game theory that quantifies the contribution that each player brings to the result of a game.\n",
    "Similarly, SHAP values quantify the contribution that each feature brings to the prediction made by a machine learning model.  \n",
    "Note that here the game is a **single observation** so keep in mind that SHAP values always depend on the machine learning model and on the observation. Indeed, SHAP is about **local interpretability** of a predictive model.  \n",
    "\n",
    "In game theory, Shapley values are calculated by averaging the contribution of every subset of players to the game. This is matematically represented by the expected value conditioned by the given subset of features. But note that, in presence of interaction, the conditioned expected values depends on the order of the conditions meaning that in order to get the exact result one should compute all the *n!* permutations of features.  \n",
    "In order to simplify the calculus, SHAP values assume that the features are **independent** of each other.\n",
    "\n",
    "**Libraries**\n",
    "- https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/interpretable-machine-learning-using-shap-theory-and-applications-26c12f7a7f1a\n",
    "- https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30#:~:text=SHAP%20%E2%80%94%20which%20stands%20for%20SHapley,output%20of%20any%20predictive%20algorithm.\n",
    "\n",
    "**Watch out**: SHAP strongly depends on some assumptions therefore you should understand them deeply in order to use SHAP. In particular\n",
    "- correlation not causality: SHAP only explains the variable correlation defined per the model structure\n",
    "- dependency on the model: SHAP by design represents how important it is a feature to the model, not how important it actually is\n",
    "- multicollinearity issue: when having variables with high degree of multicollinearity the SHAP values are high for one and very low for the remaining ones\n",
    "\n",
    "**References**\n",
    "- https://towardsdatascience.com/why-shap-values-might-not-be-perfect-cbc8056056be\n",
    "- https://towardsdatascience.com/using-shap-for-explainability-understand-these-limitations-first-1bed91c9d21#:~:text=Limitation%201%3A%20Correlation%20not%20Causality,would%20have%20causality%20as%20well\n",
    "- https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b1ee6",
   "metadata": {},
   "source": [
    "First load JS visualization code to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e662a",
   "metadata": {},
   "source": [
    "**explainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb41aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model=cbc_model, model_output='raw')\n",
    "explainer = shap.TreeExplainer(model=model, model_output='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc86e6d",
   "metadata": {},
   "source": [
    "**expected values**\n",
    "\n",
    "expected value is $E[f(x)]$ that is the expected value of the target variable, or in other words, the mean of all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e541286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explainer.expected_value)\n",
    "print(expit(explainer.expected_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d447508",
   "metadata": {},
   "source": [
    "**shap values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ad57a",
   "metadata": {},
   "source": [
    "As we said, SHAP values represent the impact of a feature for a specific observation, hence SHAP values differ depending on the chosen observation.  \n",
    "A plot that allows to get an insight of SHAP values over the full set of observatoins is **beeswarm plot**.\n",
    "\n",
    "Beeswarm plot represents the SHAP value of a feature for each observation as a point.\n",
    "\n",
    "The beeswarm plot's straight vertical line marks the null value.\n",
    "Features names are printed next to the plot for reference.\n",
    "For each feature, the shap values for each observation for the specific feature are represented as dots or as violin in an horizontal line where values that are on the right side have a high SHAP value whereas values that are on the left side have a low SHAP value.\n",
    "Each dot is colored depending on the observation value for the specific feature, blue denoting a low value for that feature and red denoting a high value insted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a99765",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(\n",
    "    shap_values=shap_values,\n",
    "    features=train_X,\n",
    "    max_display=100,\n",
    "    plot_type='dot',\n",
    "    plot_size=(30,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83218800",
   "metadata": {},
   "source": [
    "Another useful representation is the **force plot**.\n",
    "\n",
    "Force plots depicts the contribution of each feature to the model's prediction for a specific observation.  \n",
    "This allows to perfectly explain how your model arrived at the prediction it did.\n",
    "\n",
    "The force plot represents both the model's base value as well as the prediction for the given observation.\n",
    "The most significant features that are driving the prediction are printed below.\n",
    "Features that appear in red color on the left side are the ones pushing the predicted value up whereas the features that appear in blue color on the right side are the ones pulling the predicted value down.\n",
    "On each side, the most significant features are represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp = train_X[train_X.index == row.ADMIN6_CODE]\n",
    "proba = model.predict_proba(supp.iloc[0,:])[1]\n",
    "\n",
    "shap.plots.force(explainer(supp))\n",
    "\n",
    "# for a finer control over the parameters\n",
    "shap_values = explainer.shap_values(supp)\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values,                     # shap_values[row['index']],\n",
    "    features=supp.iloc[0,:],                     # train_X.iloc[row['index']],\n",
    "    feature_names=supp.columns,                  # train_X.columns,\n",
    "    text_rotation=0,\n",
    "    link='logit'                                 # identity or logit, with identity the result f(x) might be outside (0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd49830",
   "metadata": {},
   "source": [
    "You can save the plot as html file by simply assigning the plot to a variable and then using the method `save_html`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d0aab",
   "metadata": {},
   "source": [
    "Finally, the latest added plot to the SHAP library is **decision plot**.\n",
    "\n",
    "Decision plot offer a detailed view of a model prediction for a specific observation.\n",
    "\n",
    "The decision plot's straight vertical line marks the model's base value.\n",
    "The colored line instead is the prediction for each feature and feature values are printed next to the prediction line for reference.  \n",
    "Starting at the bottom of the plot, the prediction line shows how the SHAP values accumulate from the base value to arrive at the model's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a36f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SHAP decision plot for PDV {high_code} (idx {high_idx})')\n",
    "shap.decision_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=explainer.shap_values(train_X[train_X.index == high_code]),\n",
    "    feature_names=list(train_X.columns),\n",
    "    link='logit'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be92679",
   "metadata": {},
   "source": [
    "Other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot\n",
    "shap.plots.bar(explainer(supp)[0], max_display=15)\n",
    "\n",
    "# waterfall plot\n",
    "shap.plots.waterfall(explainer(supp[0, cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246470ab",
   "metadata": {},
   "source": [
    "<a name=\"lift\"></a>\n",
    "## Lift curve\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03328b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "773da3f5",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"tf\"></a>\n",
    "# TensorFlow\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce482fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb257a",
   "metadata": {},
   "source": [
    "TensorFlow 2 by default has eager execution enabled that makes TensorFlow functions execute operations immediately as opposed to adding to a graph to be executed later in a tf.compat.v1.Session and return concrete values as opposed to symbolic references to a node in a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed5039",
   "metadata": {},
   "source": [
    "Define TensorFlow edges: variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants are immutable values which do not change during a computation process\n",
    "A = tf.constant(5.5, name='constant_a')\n",
    "B = tf.constant(3., name='constant_b')\n",
    "C = tf.constant(0.7, name='constant_c')\n",
    "\n",
    "# placeholders are values assigned once and that do not change afterwars\n",
    "# placeholders in tensorflow do not have input values even though you can specify some if you want\n",
    "# they serve as input nodes, they are actually tensors which do not take their values until we execute the graph\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=[3], name='x')\n",
    "y = tf.compat.v1.placeholder(tf.float32, shape=[3], name='y')\n",
    "\n",
    "# variables are values that can be constantly changed or recomputed\n",
    "m = tf.Variable([2.5, 4.0, 2.0], tf.float32, name='var_m')\n",
    "c = tf.Variable([5.0, 10.0, 3.5], tf.float32, name='var_c')\n",
    "number = tf.Variable(4.5)\n",
    "multiplier = tf.Variable(1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca1cd1",
   "metadata": {},
   "source": [
    "If you use variables in your TensorFlow program you need to run a special command to initialize variables.  \n",
    "Variables cannot be used before running this specific command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to initialize all variables in your code\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# to only initialize a bunch of selected variables\n",
    "tf.compat.v1.variables_initializer([m, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00806d3f",
   "metadata": {},
   "source": [
    "Define TensorFlow nodes: operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07638074",
   "metadata": {},
   "outputs": [],
   "source": [
    "add = tf.add(A, B, name='add_ab')\n",
    "subtract = tf.subtract(B, C, name='subtract_bc')\n",
    "square = tf.square(C, name='square_c')\n",
    "final_sum = tf.add_n([add, subtract, square], name='final_sum')\n",
    "\n",
    "sum_x = tf.reduce_sum(x, name='sum_x')\n",
    "prod_y = tf.reduce_prod(y, name='prod_y')\n",
    "final_div = tf.math.divide(sum_x, prod_y, name='final_div')\n",
    "\n",
    "# an operation on variables and placeholders will also be a variable\n",
    "# NOTE: to compute a combined operation of variables and placeholders, they must have the same shape\n",
    "line = m * x + c\n",
    "\n",
    "result = number.assign(tf.multiply(number, multiplier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81cb05",
   "metadata": {},
   "source": [
    "Create TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e70992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: remember to instantiate the TensorFlow session using with operator in order to be sure it is automatically shut down when you are done\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # run init to initialize all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(f'sum(x): {sess.run(sum_x, feed_dict={x: [100,200,300]})}')\n",
    "    print(f'prod(y): {sess.run(prod_y, feed_dict={y: [11,22,33]})}')\n",
    "    print(f'sum(x)/prod(y): {sess.run(final_div, feed_dict={x: [100,200,300], y: [11,22,33]})}', end='\\n\\n')\n",
    "    \n",
    "    print(f'Line mx + c: {sess.run(line, feed_dict={x: [0, 1, 10]})}', end='\\n\\n')\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(f'Result number*multiplier: {sess.run(result)}')\n",
    "        print(f'Increase multiplier value: {sess.run(multiplier.assign_add(1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f2096",
   "metadata": {},
   "source": [
    "To visualize computation graph using TensorBoard you need to use tf.summary.FileWriter to save data in a target folder and then initialize TensorBoard on that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ebc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('./TensorFlowOutput', sess.graph)\n",
    "writer.close()\n",
    "\n",
    "# WATCH OUT: this is google.datalab.ml code, however Datalab has been shutdown by google\n",
    "tensorboard_pid = ml.TensorBoard.start('./TensorFlowOutput')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c8eb0",
   "metadata": {},
   "source": [
    "Once you're done close the TensorBoard session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3359489",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.TensorBoard.stop(tensorboard_pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc29d0f",
   "metadata": {},
   "source": [
    "<a name=\"tf_mnist\"></a>\n",
    "## MNIST\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dabd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a feed forward neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run the model on a single data point\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure and compile the model specifying an optimizer, the loss function and an evaluation metric\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to retrieve probabilities you can wrap the model with the softmax function\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "probability_model(x_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
