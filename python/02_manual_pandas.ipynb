{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e07779",
   "metadata": {},
   "source": [
    "# Pandas manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0780a",
   "metadata": {},
   "source": [
    "<a name=\"contents\"></a>\n",
    "# Contents\n",
    "\n",
    "- [Configuration](#conf)\n",
    "- [Functions](#fnc)\n",
    "- [Pandas](#pandas)\n",
    "    - [simple df I/O](#simpleio)\n",
    "    - [simple df handling](#handling)\n",
    "    - [reshape df](#reshape)\n",
    "    - [slice df](#slice)\n",
    "    - [binning](#binning)\n",
    "    - [row-wise operations](#rowwise)\n",
    "        - [replace](#replace)\n",
    "        - [map](#map)\n",
    "        - [apply](#apply)\n",
    "    - [aggregate operations](#aggregate)\n",
    "        - [groupby](#groupby)\n",
    "        - [aggregation](#aggr)\n",
    "        - [transformation](#transf)\n",
    "        - [filter](#filter)\n",
    "    - [combining operations](#combine)\n",
    "        - [concat](#concat)\n",
    "        - [join](#join)\n",
    "        - [merge](#merge)\n",
    "    - [encoding](#encoding)\n",
    "    - [complex operations](#complex)\n",
    "- [Geopandas](#gpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1729864",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"conf\"></a>\n",
    "# Configuration\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import pdb\n",
    "import json\n",
    "import pytz\n",
    "import time\n",
    "import string\n",
    "import inspect\n",
    "import zipfile\n",
    "import unittest\n",
    "import random as rnd\n",
    "import datetime as dt\n",
    "import multiprocessing\n",
    "from operator import itemgetter\n",
    "from itertools import groupby, chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from pympler import asizeof\n",
    "from IPython.display import display, HTML, Image\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518867e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "display(HTML('<style>.container { width:90% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fbe24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name=\"fnc\"></a>\n",
    "# Functions\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721fcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e936b61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name=\"pandas\"></a>\n",
    "# Pandas\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df77963",
   "metadata": {},
   "source": [
    "<a name=\"simpleio\"></a>\n",
    "### simple df I/O\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas series\n",
    "series = pd.Series({'a': 1, 'b': 2, 'c': 3})\n",
    "\n",
    "# series operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f6bab",
   "metadata": {},
   "source": [
    "#### create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182dad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T10:58:06.489626Z",
     "start_time": "2022-08-26T10:58:06.457675Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataframe can be column-wise defined\n",
    "df = pd.DataFrame({\n",
    "    'index': ['user0', 'user1', 'user2', 'user3', 'user4', 'user5', 'user6', 'user7', 'user8', 'user9'],\n",
    "    'name': ['Marco', 'John', 'Alejandro', 'Kylie', 'Abdul', 'Ingrid', 'Francois', 'Ilaria', 'Jesse', 'Tina'],\n",
    "    'surname': ['Rossi', 'Doe', 'Lara', 'Liu', 'Said', 'Larsson', 'Degass', 'Rossi', 'Pinkman', 'Orwell'],\n",
    "    'age': [32, 28, 41, 36, 39, 19, 22, 229, 31, 28],\n",
    "    'email': ['m.rossi@gmail.com', 'j.doe@gmail.com', 'a.lala@gmail.com', np.NaN, 'a.said@gmail.com', 'i.larsson@gmail.com', 'f.degass@gmail.com', 'i.rossi@gmail.com', 'jpkm89@hotmail.com', 'orwellt@yahoo.com'],\n",
    "    'attended': [True, True, False, True, True, False, True, True, False, False],\n",
    "    'class': ['B2', 'A2', np.NaN, 'B1','C2', np.NaN, 'B2', 'B1', np.NaN, np.NaN],\n",
    "    'avg_score': [3.2, 3.3, np.NaN, 2.9, 'three', np.NaN, 3.9, 3.5, np.NaN, np.NaN]})\n",
    "\n",
    "# alternatively you can also define it row-wise\n",
    "data = [\n",
    "    ['user6', 'Ingrid', 'Larsson', 19, 'i.larsson@gmail.com', False, np.NaN, np.NaN],\n",
    "    ['user7', 'Francois', 'Degass', 22, 'f.degass@gmail.com', True, 'B2', 3.9],\n",
    "    ['user8', 'Ilaria', 'Rossi', 229, 'i.rossi@gmail.com', True, 'B1', 3.5]\n",
    "]\n",
    "columns = ['index', 'name', 'surname', 'age', 'email', 'attended', 'class', 'avg_score']\n",
    "df1 = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "df2 = pd.DataFrame({'sex': [1, 1, 1, 0, 1, 0, 1, 0]})\n",
    "\n",
    "# you can also specify index from a list or from an existing dataframe\n",
    "df1 =  pd.DataFrame(data, columns=columns, index=range(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25b9e1",
   "metadata": {},
   "source": [
    "#### write df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "# write pandas df to csv\n",
    "df.to_csv('output.csv', index=False)\n",
    "# write pandas df containing special characters to csv\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# write df to json or other weird formats\n",
    "with open(output_path, 'wb') as file:\n",
    "    dump(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad024649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel\n",
    "# write pandas df to new excel\n",
    "df.to_excel('output.xlsx', sheet_name='sheet', index=False)\n",
    "# write pandas df containing special characters to excel\n",
    "df.to_excel(output_path, engine='xlsxwriter', index=False)\n",
    "\n",
    "# write multiple pandas df to same new excel\n",
    "with pd.ExcelWriter('output.xlsx') as writer:\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    model_dependencies.to_excel(writer, sheet_name='model_dependencies', index=False)\n",
    "    writer.save()\n",
    "    \n",
    "# write pandas df to existing excel\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook('EmailAssistsCfr.xlsx')\n",
    "writer = pd.ExcelWriter('EmailAssistsCfr.xlsx')\n",
    "writer.book = book\n",
    "df.to_excel(writer, sheet_name='EmailAssistedLeads', index=False, header=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "# write excel containing unicode\n",
    "os.cmd('pip install xlsxwriter')\n",
    "df.to_excel('test.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148bfd1",
   "metadata": {},
   "source": [
    "#### read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv('Data/Titanic/train.csv')\n",
    "# read encoded csv file\n",
    "df = pd.read_csv('anl_CRY_FullClientList_clientContactInfo_V2.csv', sep=';', encoding='ISO-8859-1')\n",
    "\n",
    "# read excel\n",
    "df = pd.read_excel(path, usecols=['id', 'name', 'cellphone'])\n",
    "\n",
    "# read multiple sheets from excel\n",
    "dfs = []\n",
    "for sheet in sheets:\n",
    "    df = pd.read_excel(file_name, sheet_name=sheet, usecols=[cols_to_read])\n",
    "    dfs.append(df)\n",
    "dfs = pd.concat(dfs).drop_duplicates()\n",
    "\n",
    "# read multiple files\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, usecols=columns_list)\n",
    "    dfs.append(df)\n",
    "dfs = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270ee32",
   "metadata": {},
   "source": [
    "<a name=\"handling\"></a>\n",
    "### simple df handling\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88349b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- DESCRIPTIVE STATISTICS\n",
    "df.dtypes                                                        # check columns types\n",
    "df.head(n)                                                       # select first n rows\n",
    "df.tail(n)                                                       # select last n rows\n",
    "df.sample(n)                                                     # select random subset of n rows\n",
    "df.describe()                                                    # generate descriptive statistics about df\n",
    "df.columns                                                       # show list of columns\n",
    "df.columns[df.dtypes=='object']                                  # find categoric / numeric columns\n",
    "pd.factorize(df[col])                                            # map categorical values to integers, extract unique values\n",
    "\n",
    "# FOCUS: describe\n",
    "df.describe([.01,.05,.25,.5,.75,.9,.95,.99])\n",
    "df.describe(include=['object'])\n",
    "df.describe(include=['category'])\n",
    "df.describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- RESHAPE DATAFRAME\n",
    "df.set_index(col)                                                # set col as index\n",
    "df.reset_index()                                                 # reset index to row nr\n",
    "df.index.name = None                                             # remove index name\n",
    "df.rename_axis(None)                                             # remove axis name\n",
    "df.transpose()                                                   # transpose df\n",
    "df.reindex(idx) / df.reindex(idx, axis=1)                        # conform df to new index with optional filling logic (can be done on row indexes or on columns)\n",
    "pd.melt(df)                                                      # gather columns into rows\n",
    "df.rename({col:new_col}, axis=1)                                 # rename columns\n",
    "df.drop([col1, col2], axis=1, errors='ignore')                   # drop list of columns\n",
    "\n",
    "# FOCUS: index\n",
    "# to change the index and then restore the index level you have to remove index name after resetting it\n",
    "df = df.set_index(col)\n",
    "df.index.name = None\n",
    "# creating a multi-index\n",
    "# below example shows how to create a column multi-index through a dictionary d, however the same can be done with rows\n",
    "df.columns = pd.MultiIndex.from_tuples([(d[k], k) for k in df.columns])\n",
    "# to concatenate multi-indexes into one\n",
    "df.columns.map('_'.join).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e76783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- SORT DATAFRAME\n",
    "df.sort_index()                                                  # sort by index\n",
    "df.sort_values(by=[col1, col2], ascending=False)                 # sort by given columns\n",
    "df.sort_values([(lvl1, lvl2)])                                   # sort by multi-index\n",
    "\n",
    "# sort by custom list\n",
    "sort_list = []\n",
    "df.column = df.column.astype('category')\n",
    "df.column.cat.set_categories(sort_list, inplace=True)\n",
    "df = df.sort_values(['column'])\n",
    "\n",
    "# FOCUS: sorting with multi-index\n",
    "# here you can specify whatever number of index levels and ordering\n",
    "# let's first create a multi-index and then sort it\n",
    "df.columns = pd.MultiIndex.from_tuples([(d[k], k) for k in df.columns])\n",
    "df.sort_index(axis=1, level=[0, 1], ascending=[True, False], inplace=True)\n",
    "\n",
    "# FOCUS: change data type\n",
    "df[col] = df[col].astype('category')\n",
    "df[col] = df[col].astype('int')\n",
    "df[col] = df[col].astype('float')\n",
    "# to convert a column containing nans to int use instead Int64\n",
    "df[col] = df[col].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- SLICE DATAFRAME\n",
    "# the simplest way to slice a pandas dataframe is using a boolean series\n",
    "# the usage does not depend on the index axis (whether is it the default index or a user defined index)\n",
    "df[df['age'] > 30]\n",
    "df[df['age'].isin([20, 25, 30, 35])]\n",
    "df[df['age'].between(18, 24)]\n",
    "\n",
    "# label-based slicing\n",
    "# loc is label-based meaning that rows and columns have to be specified by their names\n",
    "df.loc[index_val]                                                # scalar indexing\n",
    "df.loc[index_vals]                                               # list indexing\n",
    "df.loc[bool_list]                                                # list of booleans indexing: it has to be the same length of the dataframe!\n",
    "df.loc[index_val, 'age']                                         # indexing both axis\n",
    "df.loc[df['age'] > 30, 'email']                                  # series indexing\n",
    "\n",
    "# index-based slicing\n",
    "# iloc is integer index-based meaning that rows and columns have to be specified by their integer indexes\n",
    "# this is useful when you know the index you are interested in or when you are looping throung the dataframe\n",
    "df.iloc[1]                                                       # scalar integer indexing: this will return a Series obj\n",
    "df.iloc[[1]]                                                     # list of integers indexing: this will return a DataFrame obj\n",
    "df.iloc[bool_list]                                               # list of booleans indexing: it has to be the same length of the dataframe!\n",
    "df.iloc[1, 2]                                                    # indexing both axis\n",
    "# NB iloc does not work with Series indexing, to solve this you have to convert the Series to list\n",
    "df.iloc[(df['age'] > 20).to_list()]\n",
    "\n",
    "# other slicing techniquest\n",
    "# slicing based on map functions\n",
    "df[df['list'].map(len) > 1]\n",
    "# select rows based on regex\n",
    "df.filter(regex='e$', axis=1)\n",
    "# select n largest / smallest rows\n",
    "df.nlargest(5, 'age')\n",
    "df.nsmallest(5, 'age')\n",
    "\n",
    "# FOCUS: slicing with multi-index\n",
    "df.columns = pd.MultiIndex.from_tuples([(d[k], k) for k in df.columns])\n",
    "# the first method allows to slice rows\n",
    "df[df[('idx_lvl1','idx_lvl2')].isin([2805803003,2806501014])]\n",
    "# otherwise you can take advantage of pandas loc and indexSlice methods to slice rows and columns\n",
    "idx = pd.IndexSlice\n",
    "df.loc[:, idx[['a_secci√≥n','renta'],:]]\n",
    "# WIP\n",
    "# selezionare una colonna da un indice e tutte le colonne da un altro indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- MISSING VALUES\n",
    "df.drop_duplicates()                                             # drop duplicated rows\n",
    "\n",
    "# counting missing values\n",
    "df.isna().sum()\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# visualising missing values\n",
    "df[df['class'].isna()].head()\n",
    "df[df['class'].isnull()]\n",
    "df[df.isna()].head()\n",
    "\n",
    "# filling missing values in a column\n",
    "df['avg_score'] = df['avg_score'].fillna(0)\n",
    "\n",
    "# filling missing values in all dataframe\n",
    "# fill with user defined value like empty string\n",
    "df = df.fillna('')\n",
    "# to fill with median\n",
    "replace_dict = df.median().to_dict()\n",
    "df= df.fillna(replace_dict)\n",
    "# to fill with values from another column\n",
    "df['name'] = df['name'].fillna(df['surname'])\n",
    "\n",
    "# drop missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- DUPLICATED VALUES\n",
    "# get first occurrence of duplicated values from column\n",
    "df[df.duplicated(subset='name')]\n",
    "df[df.duplicated(subset=['name', 'surname'])]\n",
    "\n",
    "# get all rows that have duplicates in column\n",
    "df[df['surname'].isin(df[df.duplicated(subset='surname')]['surname'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date from string columns\n",
    "df['SendDate'] = df['SendDate'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
    "df['SendDate'] = df['SendDate'].dt.date\n",
    "\n",
    "# fill date columns\n",
    "df['SendDate'] = df['SendDate'].fillna(dt.date(1900, 1, 1))\n",
    "\n",
    "# extract info from date\n",
    "df['Year'] = df['SendDate'].apply(lambda x: x.strftime('%Y'))\n",
    "df['Month'] = df['SendDate'].apply(lambda x: x.strftime('%B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c8279",
   "metadata": {},
   "source": [
    "<a name=\"reshape\"></a>\n",
    "### reshape df\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19824c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7424fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread rows into columns\n",
    "# here index denotes the column that will be kept as index (MUST be single index)\n",
    "# columns denotes the columns that will be used as columns\n",
    "# values will be the values placed inside the table\n",
    "df_pivot = df.pivot(index=['name','surname'], columns='class', values='avg_score')\n",
    "\n",
    "# after pivoting you may want to concatenate indexes in order to have them in one row\n",
    "# this can be done with the following\n",
    "# df_pivot = df_pivot.columns.map('_'.join).str.lower()\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"inverse\" operation of a pivot is melt statement\n",
    "#  - id_vars: columns to be used as row identifier, these columns not be moved\n",
    "#  - value_vars: columns to unpivot, these columns' names will be placed in a column named 'variable'\n",
    "# the values in value_vars will be then used to populate a single column\n",
    "df_metl = pd.melt(df_pivot.reset_index(), id_vars=['name','surname'], value_vars=['A2', 'B1', 'B2', 'C2'])\n",
    "df_metl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b9131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T09:18:38.311161Z",
     "start_time": "2022-08-03T09:18:38.289869Z"
    }
   },
   "source": [
    "<a name=\"slice\"></a>\n",
    "### slice df\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c068a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can slice a dataframe with custom methods through apply method\n",
    "# note that apply requires the column to be a iterable (list, set, tuple), if that is not the case make sure to convert it to iterale before slicing\n",
    "df[df.mq_range_list.apply(lambda x: bool(set(x) & set(l)))].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99505c",
   "metadata": {},
   "source": [
    "#### loc\n",
    "\n",
    "Pandas `loc[]` allows to access a DataFrame's group of rows and columns by label(s) or a boolean array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)]                                                  # access DataFrame's last row\n",
    "df.loc[len(df), 'name']                                          # access a specific value\n",
    "df.loc[df['age'] < 50, ['name','email']]                         # select subset of rows and columns\n",
    "\n",
    "# loc can be used to modifiy data in a subset of rows\n",
    "df.loc[df['f_outlier'] == 1, 'fatturato'] = np.NaN\n",
    "\n",
    "# select a subset of dataframe based on indexes\n",
    "df = y.loc[X.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819de423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T09:18:51.356404Z",
     "start_time": "2022-08-03T09:18:51.341951Z"
    }
   },
   "source": [
    "#### iloc\n",
    "\n",
    "Pandas `iloc[]` property is a purely integer-location based indexing for selection by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5b6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T10:59:13.471050Z",
     "start_time": "2022-08-26T10:59:13.450549Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0]                                                       # access DataFrame's first row\n",
    "df.loc[len(df)]                                                  # access DataFrame's last row\n",
    "df.iloc[0]['name']                                               # access a specific value\n",
    "\n",
    "# select a subset of dataframe based on indexes\n",
    "df = y.iloc[X.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da32552",
   "metadata": {},
   "source": [
    "### binning\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6319c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static binning\n",
    "# function cut splits a serie in bins according to pre-defined (static) bins\n",
    "bins = [0, 2, 3.1, 5]\n",
    "df['comment'] = pd.cut(df['avg_score'], bins)\n",
    "\n",
    "# quantile binning\n",
    "# quantile-cut applies a number q of quantile splits to the serie\n",
    "df['quantile_comment'] = pd.qcut(df['avg_score'], q=nr_bins, labels=comment)\n",
    "# qcut actually returns the intervals (pandas.Interval) as you can see below\n",
    "df['age_qbins'] = pd.qcut(df.age, q=3).values\n",
    "# use methods such as left, right to access each interval's boundary\n",
    "df['age_boundary'] = df['age_qbins'].apply(lambda x: x.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6542f5c",
   "metadata": {},
   "source": [
    "<a name=\"rowwise\"></a>\n",
    "### row-wise operations\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column based on regex\n",
    "# extract: returns first match, if it does not match returns nan\n",
    "df['test'] = df['hdfs'].str.extract(r'name\\=(.+)')\n",
    "# findall: returns list of matches, if it does not match returns empty list\n",
    "df['test'] = df['hdfs'].str.findall(r'name\\=(.+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd069ba",
   "metadata": {},
   "source": [
    "#### replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d943e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values statically\n",
    "# use alternatively\n",
    "df['avg_score'].replace({'three': 3.}, inplace=True)\n",
    "df['avg_score'] = df['avg_score'].replace({0: 0.})\n",
    "\n",
    "# replace multiple values with the same one\n",
    "df['age'] = df['age'].replace([14, 15, 16, 17], 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3f83d",
   "metadata": {},
   "source": [
    "<a name=\"map\"></a>\n",
    "#### map\n",
    "\n",
    "Map is a Series operation that maps values of Series according to input correspondence\n",
    "\n",
    "**Note** that `map()` method is actually not a row-wise operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].map({'Miss':'Mrs', 'Ms':'Mrs'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f09722",
   "metadata": {},
   "source": [
    "<a name=\"apply\"></a>\n",
    "#### apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb69b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply simple row-wise operation to column\n",
    "df[new_column] = df[old_column].apply(lambda x: ' '.join(x.split('|')))\n",
    "\n",
    "# you can also apply row-wise operations that operate over multiple columns by applying directly to df\n",
    "# in order to do that you have to specify axis=1 to apply function\n",
    "df[new_column] = df.apply(lambda x: x.first_column if x.second_column else 'NA', axis = 1)\n",
    "\n",
    "# you can also define custom logics by writing your own functions\n",
    "def getTech(legend):\n",
    "        tech = None\n",
    "        tech_initial = legend[1]\n",
    "        if(tech_initial == 'L') :\n",
    "            tech = 'LTE'\n",
    "        elif(tech_initial == 'U') :\n",
    "            tech = 'UMTS'\n",
    "        elif(tech_initial == 'G') :\n",
    "            tech = 'GSM'\n",
    "        return tech\n",
    "df['tech'] = df.apply(lambda x: getTech(x['legend']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54214aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create multiple columns at once with apply\n",
    "def get_cell(legend):\n",
    "    legend_inital = legend[1]\n",
    "    if legend_inital in ['D', 'G', 'M', 'N']:\n",
    "        tech = 'GSM'\n",
    "        gen = '2G'\n",
    "        zci = legend[0] + legend[2:7] + legend[-1]\n",
    "    elif legend_inital in ['H', 'U', 'V', 'X']:\n",
    "        tech = 'UMTS'\n",
    "        gen = '3G'\n",
    "        zci = legend[0] + legend[2:7] + legend[-1]\n",
    "    elif legend_inital in ['L']:\n",
    "        tech = 'LTE'\n",
    "        gen = '4G'\n",
    "        zci = legend[6:11] + '0' + legend[14:16]\n",
    "    else:\n",
    "        tech = 'N/A'\n",
    "        gen = 'N/A'\n",
    "        zci = 'N/A'\n",
    "    return pd.Series([tech, gen, zci])\n",
    "\n",
    "# mtd 1: return a pd.Series\n",
    "coverage[['TECH', 'GEN', 'ZCI']] = coverage['LEGEND'].apply(get_cell)\n",
    "\n",
    "# mtd 2: use zip\n",
    "scraping_out['hours_info'], scraping_out['totale_ore'], scraping_out['orario_continuato'], scraping_out['aperto_24h'] = zip(*scraping_out['hours'].apply(extract_hour_info))\n",
    "\n",
    "# split column into multiple columns\n",
    "df[['codice11', 'codice11_dest']] = pd.read_table(io.StringIO(df['Name'].to_csv(None, index=False, header=False)), sep=' - ', dtype=str, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e337f8",
   "metadata": {},
   "source": [
    "<a name=\"aggregate\"></a>\n",
    "### aggregate operations\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226f894",
   "metadata": {},
   "source": [
    "<a name=\"groupby\"></a>\n",
    "#### groupby\n",
    "\n",
    "Generally, a `groupby` function consists of three steps:\n",
    "- split\n",
    "- apply\n",
    "- combine\n",
    "\n",
    "The split step breaks up the dataframe into subset dataframes based on the specified keys.  \n",
    "Then, apply step applies functions to those subset dataframes.  \n",
    "Last, combine step concatenates those results into an output array.\n",
    "\n",
    "**Note** that groupby method returns a pandas DataFrameGroupBy object with the group column(s) as index.\n",
    "Remember to use `reset_index` after groupby if you want to have group columns on the same level as the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple groupby\n",
    "df.groupby(['Pclass']).mean()\n",
    "\n",
    "# grouping on multiple columns\n",
    "df.groupby(['Sex', 'Pclass']).mean()\n",
    "\n",
    "# resetting index\n",
    "df.groupby(['Pclass']).mean().reset_index()\n",
    "df.groupby(['Pclass'], as_index=False).mean()\n",
    "\n",
    "# imputing missing values\n",
    "df['Deck'] = df.groupby(by=['Ticket'], sort=False)['Deck'].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "\n",
    "# handling missing values (pandas 1.3.3)\n",
    "# df_missing = df.copy()\n",
    "# df_missing.iloc[80:100, 0] = np.nan\n",
    "# df_missing.groupby(by='Pclass', dropna=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42058bb",
   "metadata": {},
   "source": [
    "<a name=\"aggr\"></a>\n",
    "#### aggregation\n",
    "\n",
    "Aggregation means that an aggregate DataFrame will be retreived with a number of rows matching the number of classes in the column(s) on which the `goupby` is performed.  \n",
    "Basically aggregation allows to apply multiple functions to a subset of columns and can be done in pandas with the `aggr` method.\n",
    "\n",
    "The aggregation logical steps are the following.  \n",
    "First, it splits the full dataframe into sub DataFrames based on the grouping column(s).  \n",
    "Then it applies aggregation function(s) to the specified column(s).  \n",
    "Last, it combines the results into series that can be converted to DataFrame through `reset_index`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f42315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple operations\n",
    "# NOTE that if you specify a column, like Age in the following, you get aggregations only on that specific one\n",
    "# you can get aggregations on all columns if you avoid specifying one\n",
    "df.groupby('Sex').Age.max()\n",
    "\n",
    "# agg method\n",
    "df.groupby('Sex').Age.agg(['max', 'min', 'count', 'median', 'mean'])\n",
    "# to aggregate multiple columns with custom operations\n",
    "df.groupby('Sex').agg({'Age': ['min', 'max'], 'Fare': 'sum'})\n",
    "# to rename columns instead use the following method\n",
    "df.groupby('Sex').Age.agg(sex_max=('max'), sex_min=('min'))\n",
    "df.groupby('Sex').agg(age_max=('Age','max'), age_min=('Age','min'), fare_sum=('Fare','sum'))\n",
    "\n",
    "# nice functions\n",
    "# numpy ptp (peak to peak) retrieves the range of values (maximum - minimum) along an axis\n",
    "df.groupby('Sex').agg(age_delta=('Age',np.ptp), fare_delta=('Fare',np.ptp))\n",
    "\n",
    "# to use custom functions\n",
    "def categorize(x):\n",
    "    return True if x.mean() > 29 else False\n",
    "df.groupby('Sex').Age.agg(['max', 'mean', categorize])\n",
    "\n",
    "# or with lambda expression\n",
    "df.groupby('Sex').Age.agg(['max', 'mean', lambda x: True if x.mean() > 50 else False])\n",
    "\n",
    "# alternatively use APPLY method when performing aggregation on whole column and NOT on single elements (see below)\n",
    "df.groupby('Pclass').mean().head()\n",
    "df.groupby('Pclass').apply(lambda x: x.mean()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee68d1",
   "metadata": {},
   "source": [
    "<a name=\"transf\"></a>\n",
    "#### transformation\n",
    "\n",
    "Transformation returns a list-indexed object with the same number of rows as the input DataFrame and can be done in pandas with the `transform` method.\n",
    "\n",
    "Compared with aggregation, transformation takes an additional step called *Broadcasting*.\n",
    "Broadcasting consists of returning the results from sub DataFrames to the original full DataFrame.\n",
    "Thus transform will always return a series with the same length to the original full DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d63ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Pclass').transform(lambda x: x.mean()).head()\n",
    "\n",
    "# to use custom functions\n",
    "standardization = lambda x: (x - x.mean()) / x.std()\n",
    "df.groupby('Sex').Age.transform(standardization)\n",
    "\n",
    "# NOTE that when the apply function is applied to single element the it behaves exactly as the transform function\n",
    "df.groupby('Sex').Age.apply(standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb91fe8",
   "metadata": {},
   "source": [
    "<a name=\"filter\"></a>\n",
    "#### filter\n",
    "\n",
    "Finally, filtering means discarding some observations according to a group-wise computation that evaluates True or False.\n",
    "Filtering in pandas is done by the `filter` method.\n",
    "\n",
    "Compared with `transform`, `filter` takes another additional step.  \n",
    "After getting results from those sub DataFrames, it applies a filter condition to those results and then it broadcasts the result to the matching conditions in the original DataFrame.  \n",
    "Basically it is simply reducing the original DataFrame's dimension according to a grouping condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple filter\n",
    "df.groupby('Cabin').filter(lambda x: len(x) >= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837a61b",
   "metadata": {},
   "source": [
    "<a name=\"combine\"></a>\n",
    "### combining operations\n",
    "\n",
    "[Return to Contents](#contents)\n",
    "\n",
    "Pandas supports different methods to combine dataframes such as `join`, `merge` and `concat`.\n",
    "\n",
    "Both `join` and `merge` can be used to combines two dataframes but they are slightly different: `join` method combines two dataframes on the basis of their indexes whereas `merge` method is more versatile and allows user to specify columns beside the index to join on for both dataframes.  \n",
    "`concat` method on the other side allows to concatenate a list of pandas objects along a particular axis with optional set logic along the other axes. \n",
    "\n",
    "**Remember** that both `join` and `merge` reset the output dataframe index so in order to preserve the original index you have to reset it before the join or merge operation\n",
    "\n",
    "\n",
    "**References**\n",
    "- https://pandas.pydata.org/docs/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b339ef8",
   "metadata": {},
   "source": [
    "<a name=\"combine\"></a>\n",
    "#### join\n",
    "\n",
    "Among the three operations, `join` is the one that allows the lowest level of control.  \n",
    "It is usually used to join data based on their indexes and it will combine all the columns from the two tables, with the common columns renamed with the defined `lsuffix` and `rsuffix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = df1.join(df2)\n",
    "# outer join\n",
    "\n",
    "# anti join\n",
    "outer_join = left_df.merge(right_df, how='outer', indicator=True)\n",
    "# then the column _merge will be populated with the following values: both, left_only, right_only\n",
    "# hence you can derive the anti join with the following line\n",
    "anti_join = outer_join[~(outer_join._merge == 'both')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ca354",
   "metadata": {},
   "source": [
    "<a name=\"merge\"></a>\n",
    "#### merge\n",
    "\n",
    "Similarly to `join`, `merge` also combines all the columns from the two tables, with the common columns renamed with the defined suffixes however, it provides three ways of flexible control over row-wise alignment.  \n",
    "The first way is to use `on=COLUMN_NAME`, here the given column must be the common column in both tables.  \n",
    "The second way is to use `left_on=LEFT_COLUMN_NAME` and `right_on=RIGHT_COLUMN_NAME` , and it allows to align the two tables using two different columns.  \n",
    "The third way is to use `left_index=True` and `right_index=True`, and the two tables are aligned based on their index.\n",
    "\n",
    "**Note** that `merge` is more powerful that `join` resulting in some circumstances `join` not being able to actually join tables (and raining data format error) whereas `merge` can.  \n",
    "Prefer `merge` over `join` if you are not joining on DataFrames' indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455257f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe method\n",
    "left_df.merge(right_df)\n",
    "\n",
    "# function\n",
    "pd.merge(left_df, right_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357cee0",
   "metadata": {},
   "source": [
    "<a name=\"combine\"></a>\n",
    "#### concat\n",
    "\n",
    "Different from `join` and `merge`, which by default operate on columns, `concat` can define whether to operate on columns or rows.  \n",
    "By default it operates on rows `axis=0` but user can specify `axis=1` to operate on columns.\n",
    "\n",
    "`concat` is useful when generating/reading dataframes in loop with the same schema.  \n",
    "These dataframes can be stored in a list during the loop and finally be concatenated to get a unique dataframe storing all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead96afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append rows to a dataframe\n",
    "concat_df = pd.concat([df, df1])\n",
    "# use flag ignore_index=True to not use the index values along the concatenation axis\n",
    "concat_df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "# append columns to dataframe\n",
    "concat_df = pd.concat([df, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d846821",
   "metadata": {},
   "source": [
    "<a name=\"encoding\"></a>\n",
    "### encoding\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of dummy columns\n",
    "pd.get_dummies(df, columns=list_of_columns, prefix=prefix_to_encoded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25390c68",
   "metadata": {},
   "source": [
    "<a name=\"complex\"></a>\n",
    "### complex operations\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate a list of tuples and keep tuple with the maximum second element\n",
    "# NB in order to aggregate, list have to be sorted by the element on which you will be aggregating\n",
    "tuples_list = tag_arr_ = [('piero', 1), ('franco', 0.5), ('lillo', 3), ('dome', 0.1), ('franco', 2)]\n",
    "tuples_list.sort(key=lambda tup: tup[1])\n",
    "[tup for tup in [max(v, key=itemgetter(0)) for k, v in groupby(tuples_list, itemgetter(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4eb65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this functions to correct locations depending on usage\n",
    "\n",
    "# explode lists\n",
    "df.explode('column', ignore_index=True)\n",
    "\n",
    "# transpose\n",
    "df.T\n",
    "\n",
    "# automatically try to infer best data type for object column\n",
    "df.convert_dtypes()\n",
    "\n",
    "# pandas options\n",
    "# you can surf pandas options with\n",
    "dir(pd.options)\n",
    "dir(pd.options.display)\n",
    "# and then changing configuration as\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.precision = 5\n",
    "\n",
    "# pandas styler\n",
    "# you can change style to dataframes\n",
    "df.style.highlight_max(color='darkred')\n",
    "df.style.background_gradient(subset=['mean', '50%'], cmap='Reds')\n",
    "\n",
    "# pipe together custom functions\n",
    "# here drop_duplicates and remove_outliers are assumed to be custom functions with their own input and outputs\n",
    "df = df.pipe(drop_duplicates).pipe(remove_outliers, ['price', 'carat', 'depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c02d65",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"gpd\"></a>\n",
    "# Geopandas\n",
    "\n",
    "[Return to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae528e1",
   "metadata": {},
   "source": [
    "Per disegnare bounding boxes: https://arthur-e.github.io/Wicket/sandbox-gmaps3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaa973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea88323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "maps = pd.read_csv('../../Dati/vf_maps.csv')\n",
    "maps['geometry'] = maps['geometry'].apply(wkt.loads)\n",
    "maps_gpd = gpd.GeoDataFrame(maps, geometry='geometry', crs={'init': 'epsg:4326', 'no_defs': True})\n",
    "\n",
    "# read shapefile\n",
    "maps = gpd.read_file('../../Shapefiles/atoll_202012/atoll_2g_coveragemaps_20201231.shp').to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e32fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POINTS\n",
    "# create points from scratch\n",
    "point = Point(6.681902735071699, 46.02450054611408)\n",
    "\n",
    "# create points from pandas columns\n",
    "df['geometry'] = df.apply(lambda x: Point(x.longitude, x.latitude), axis=1)\n",
    "\n",
    "# POLYGONS\n",
    "# create polygon from scratch\n",
    "boundingbox_polygon = Polygon([p1, p2, p3, p4])\n",
    "\n",
    "# create polygons from shapely points\n",
    "# when you have points from a single polygon\n",
    "df['polygon'] = Polygon([(p.x, p.y)  for p in  df.point])\n",
    "# when you have points from multiple polygons (with different id)\n",
    "df = df.groupby('id')['geometry'].apply(lambda points: Polygon([(p.x, p.y)  for p in points.tolist()])).reset_index()\n",
    "\n",
    "# GPD DF\n",
    "# create gpd dataframe from scratch\n",
    "df_gpd = gpd.GeoDataFrame(gpd.GeoSeries(boundingbox_polygon), columns=['geometry'])\n",
    "# create gpd dataframe from pandas\n",
    "df_gpd = gpd.GeoDataFrame(df, geometry='polygon', crs={'init': 'epsg:4326', 'no_defs': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f17bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union geopandas dataframe\n",
    "# pd.concat trasforms to pandas df\n",
    "coverage = coverage2g.append(coverage3g).append(coverage4g)\n",
    "\n",
    "# select / drop columns\n",
    "# using df = df[[...]] will convert dataframe to pandas instead of geopandas\n",
    "df_gpd = df_gpd.drop(columns=['index_left', 'L_ID_P', 'A_ID_P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids\n",
    "df['centroid'] = df.polygon.centroid\n",
    "\n",
    "# convex hull\n",
    "df['convex_hull'] = df.polygon.convex_hull\n",
    "df['hull'] = df['geometry'].apply(lambda x: x.convex_hull)\n",
    "\n",
    "# aggregate geometries\n",
    "# dissolve will group obs by given column and then aggregate geometries\n",
    "df = df.dissolve(by='id').reset_index()\n",
    "\n",
    "# apply buffer to geometry\n",
    "def buffer(row):\n",
    "    return row.geometry.buffer(0.01, resolution=4, cap_style=3, join_style=2)\n",
    "df['geometry'] = df.apply(buffer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join\n",
    "sjoin = gpd.sjoin(maps_gpd, coverage_gpd, how='inner', op='contains')\n",
    "\n",
    "# overlay\n",
    "overlay = gpd.overlay(maps_gpd, coverage_gpd, how='intersection')\n",
    "# sort by intersection area so that largest intersection area is last\n",
    "overlay.sort_values(by='geometry', inplace=True, key=lambda col: np.array([x.area for x in col]))\n",
    "# keep largest intersection area for each cell\n",
    "overlay.drop_duplicates(subset='LEGEND', keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv containing geometry\n",
    "df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# write csv without geometry\n",
    "df.to_csv(os.path.join(output_path, 'VFA_IN_GCP_CONFIG_VF_MAPS_vf_maps_{}.csv'.format(curr_date)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3bd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
